{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ella417/NLP/blob/main/250522_3_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8vQzOle74-cJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def read_txt(txt_path):\n",
        "  with open(txt_path, 'r') as f:\n",
        "    txt_string = f.readlines()\n",
        "  return txt_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXbsjcoKIC93"
      },
      "source": [
        "# Language modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1EwAtYF4-cJ",
        "outputId": "cad34514-e450-4535-8883-d88772cc9f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-22 05:09:03--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-22 05:09:03 (8.02 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xbdOy7ia4-cK"
      },
      "outputs": [],
      "source": [
        "txt_string = read_txt('names.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSIJEvdSN3aO",
        "outputId": "1c118e27-5595-4104-f005-4bd9be91a459"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "names_list = [x.replace('\\n', '') for x in txt_string]\n",
        "len(names_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tijegdHb6RAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f32374-4d82-4072-f5f5-2fb4015168fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn',\n",
              " 'abigail',\n",
              " 'emily',\n",
              " 'elizabeth',\n",
              " 'mila',\n",
              " 'ella',\n",
              " 'avery',\n",
              " 'sofia',\n",
              " 'camila',\n",
              " 'aria',\n",
              " 'scarlett',\n",
              " 'victoria',\n",
              " 'madison',\n",
              " 'luna',\n",
              " 'grace',\n",
              " 'chloe',\n",
              " 'penelope',\n",
              " 'layla',\n",
              " 'riley',\n",
              " 'zoey',\n",
              " 'nora',\n",
              " 'lily',\n",
              " 'eleanor',\n",
              " 'hannah',\n",
              " 'lillian',\n",
              " 'addison',\n",
              " 'aubrey',\n",
              " 'ellie',\n",
              " 'stella',\n",
              " 'natalie',\n",
              " 'zoe',\n",
              " 'leah',\n",
              " 'hazel',\n",
              " 'violet',\n",
              " 'aurora',\n",
              " 'savannah',\n",
              " 'audrey',\n",
              " 'brooklyn',\n",
              " 'bella',\n",
              " 'claire',\n",
              " 'skylar',\n",
              " 'lucy',\n",
              " 'paisley',\n",
              " 'everly',\n",
              " 'anna',\n",
              " 'caroline',\n",
              " 'nova',\n",
              " 'genesis',\n",
              " 'emilia',\n",
              " 'kennedy',\n",
              " 'samantha',\n",
              " 'maya',\n",
              " 'willow',\n",
              " 'kinsley',\n",
              " 'naomi',\n",
              " 'aaliyah',\n",
              " 'elena',\n",
              " 'sarah',\n",
              " 'ariana',\n",
              " 'allison',\n",
              " 'gabriella',\n",
              " 'alice',\n",
              " 'madelyn',\n",
              " 'cora',\n",
              " 'ruby',\n",
              " 'eva',\n",
              " 'serenity',\n",
              " 'autumn',\n",
              " 'adeline',\n",
              " 'hailey',\n",
              " 'gianna',\n",
              " 'valentina',\n",
              " 'isla',\n",
              " 'eliana',\n",
              " 'quinn',\n",
              " 'nevaeh',\n",
              " 'ivy',\n",
              " 'sadie',\n",
              " 'piper',\n",
              " 'lydia',\n",
              " 'alexa',\n",
              " 'josephine',\n",
              " 'emery',\n",
              " 'julia',\n",
              " 'delilah',\n",
              " 'arianna',\n",
              " 'vivian',\n",
              " 'kaylee',\n",
              " 'sophie',\n",
              " 'brielle',\n",
              " 'madeline',\n",
              " 'peyton',\n",
              " 'rylee',\n",
              " 'clara',\n",
              " 'hadley',\n",
              " 'melanie',\n",
              " 'mackenzie',\n",
              " 'reagan',\n",
              " 'adalynn',\n",
              " 'liliana',\n",
              " 'aubree',\n",
              " 'jade',\n",
              " 'katherine',\n",
              " 'isabelle',\n",
              " 'natalia',\n",
              " 'raelynn',\n",
              " 'maria',\n",
              " 'athena',\n",
              " 'ximena',\n",
              " 'arya',\n",
              " 'leilani',\n",
              " 'taylor',\n",
              " 'faith',\n",
              " 'rose',\n",
              " 'kylie',\n",
              " 'alexandra',\n",
              " 'mary',\n",
              " 'margaret',\n",
              " 'lyla',\n",
              " 'ashley',\n",
              " 'amaya',\n",
              " 'eliza',\n",
              " 'brianna',\n",
              " 'bailey',\n",
              " 'andrea',\n",
              " 'khloe',\n",
              " 'jasmine',\n",
              " 'melody',\n",
              " 'iris',\n",
              " 'isabel',\n",
              " 'norah',\n",
              " 'annabelle',\n",
              " 'valeria',\n",
              " 'emerson',\n",
              " 'adalyn',\n",
              " 'ryleigh',\n",
              " 'eden',\n",
              " 'emersyn',\n",
              " 'anastasia',\n",
              " 'kayla',\n",
              " 'alyssa',\n",
              " 'juliana',\n",
              " 'charlie',\n",
              " 'esther',\n",
              " 'ariel',\n",
              " 'cecilia',\n",
              " 'valerie',\n",
              " 'alina',\n",
              " 'molly',\n",
              " 'reese',\n",
              " 'aliyah',\n",
              " 'lilly',\n",
              " 'parker',\n",
              " 'finley',\n",
              " 'morgan',\n",
              " 'sydney',\n",
              " 'jordyn',\n",
              " 'eloise',\n",
              " 'trinity',\n",
              " 'daisy',\n",
              " 'kimberly',\n",
              " 'lauren',\n",
              " 'genevieve',\n",
              " 'sara',\n",
              " 'arabella',\n",
              " 'harmony',\n",
              " 'elise',\n",
              " 'remi',\n",
              " 'teagan',\n",
              " 'alexis',\n",
              " 'london',\n",
              " 'sloane',\n",
              " 'laila',\n",
              " 'lucia',\n",
              " 'diana',\n",
              " 'juliette',\n",
              " 'sienna',\n",
              " 'elliana',\n",
              " 'londyn',\n",
              " 'ayla',\n",
              " 'callie',\n",
              " 'gracie',\n",
              " 'josie',\n",
              " 'amara',\n",
              " 'jocelyn',\n",
              " 'daniela',\n",
              " 'everleigh',\n",
              " 'mya',\n",
              " 'rachel',\n",
              " 'summer',\n",
              " 'alana',\n",
              " 'brooke',\n",
              " 'alaina',\n",
              " 'mckenzie',\n",
              " 'catherine',\n",
              " 'amy',\n",
              " 'presley',\n",
              " 'journee',\n",
              " 'rosalie',\n",
              " 'ember',\n",
              " 'brynlee',\n",
              " 'rowan',\n",
              " 'joanna',\n",
              " 'paige',\n",
              " 'rebecca',\n",
              " 'ana',\n",
              " 'sawyer',\n",
              " 'mariah',\n",
              " 'nicole',\n",
              " 'brooklynn',\n",
              " 'payton',\n",
              " 'marley',\n",
              " 'fiona',\n",
              " 'georgia',\n",
              " 'lila',\n",
              " 'harley',\n",
              " 'adelyn',\n",
              " 'alivia',\n",
              " 'noelle',\n",
              " 'gemma',\n",
              " 'vanessa',\n",
              " 'journey',\n",
              " 'makayla',\n",
              " 'angelina',\n",
              " 'adaline',\n",
              " 'catalina',\n",
              " 'alayna',\n",
              " 'julianna',\n",
              " 'leila',\n",
              " 'lola',\n",
              " 'adriana',\n",
              " 'june',\n",
              " 'juliet',\n",
              " 'jayla',\n",
              " 'river',\n",
              " 'tessa',\n",
              " 'lia',\n",
              " 'dakota',\n",
              " 'delaney',\n",
              " 'selena',\n",
              " 'blakely',\n",
              " 'ada',\n",
              " 'camille',\n",
              " 'zara',\n",
              " 'malia',\n",
              " 'hope',\n",
              " 'samara',\n",
              " 'vera',\n",
              " 'mckenna',\n",
              " 'briella',\n",
              " 'izabella',\n",
              " 'hayden',\n",
              " 'raegan',\n",
              " 'michelle',\n",
              " 'angela',\n",
              " 'ruth',\n",
              " 'freya',\n",
              " 'kamila',\n",
              " 'vivienne',\n",
              " 'aspen',\n",
              " 'olive',\n",
              " 'kendall',\n",
              " 'elaina',\n",
              " 'thea',\n",
              " 'kali',\n",
              " 'destiny',\n",
              " 'amiyah',\n",
              " 'evangeline',\n",
              " 'cali',\n",
              " 'blake',\n",
              " 'elsie',\n",
              " 'juniper',\n",
              " 'alexandria',\n",
              " 'myla',\n",
              " 'ariella',\n",
              " 'kate',\n",
              " 'mariana',\n",
              " 'lilah',\n",
              " 'charlee',\n",
              " 'daleyza',\n",
              " 'nyla',\n",
              " 'jane',\n",
              " 'maggie',\n",
              " 'zuri',\n",
              " 'aniyah',\n",
              " 'lucille',\n",
              " 'leia',\n",
              " 'melissa',\n",
              " 'adelaide',\n",
              " 'amina',\n",
              " 'giselle',\n",
              " 'lena',\n",
              " 'camilla',\n",
              " 'miriam',\n",
              " 'millie',\n",
              " 'brynn',\n",
              " 'gabrielle',\n",
              " 'sage',\n",
              " 'annie',\n",
              " 'logan',\n",
              " 'lilliana',\n",
              " 'haven',\n",
              " 'jessica',\n",
              " 'kaia',\n",
              " 'magnolia',\n",
              " 'amira',\n",
              " 'adelynn',\n",
              " 'makenzie',\n",
              " 'stephanie',\n",
              " 'nina',\n",
              " 'phoebe',\n",
              " 'arielle',\n",
              " 'evie',\n",
              " 'lyric',\n",
              " 'alessandra',\n",
              " 'gabriela',\n",
              " 'paislee',\n",
              " 'raelyn',\n",
              " 'madilyn',\n",
              " 'paris',\n",
              " 'makenna',\n",
              " 'kinley',\n",
              " 'gracelyn',\n",
              " 'talia',\n",
              " 'maeve',\n",
              " 'rylie',\n",
              " 'kiara',\n",
              " 'evelynn',\n",
              " 'brinley',\n",
              " 'jacqueline',\n",
              " 'laura',\n",
              " 'gracelynn',\n",
              " 'lexi',\n",
              " 'ariah',\n",
              " 'fatima',\n",
              " 'jennifer',\n",
              " 'kehlani',\n",
              " 'alani',\n",
              " 'ariyah',\n",
              " 'luciana',\n",
              " 'allie',\n",
              " 'heidi',\n",
              " 'maci',\n",
              " 'phoenix',\n",
              " 'felicity',\n",
              " 'joy',\n",
              " 'kenzie',\n",
              " 'veronica',\n",
              " 'margot',\n",
              " 'addilyn',\n",
              " 'lana',\n",
              " 'cassidy',\n",
              " 'remington',\n",
              " 'saylor',\n",
              " 'ryan',\n",
              " 'keira',\n",
              " 'harlow',\n",
              " 'miranda',\n",
              " 'angel',\n",
              " 'amanda',\n",
              " 'daniella',\n",
              " 'royalty',\n",
              " 'gwendolyn',\n",
              " 'ophelia',\n",
              " 'heaven',\n",
              " 'jordan',\n",
              " 'madeleine',\n",
              " 'esmeralda',\n",
              " 'kira',\n",
              " 'miracle',\n",
              " 'elle',\n",
              " 'amari',\n",
              " 'danielle',\n",
              " 'daphne',\n",
              " 'willa',\n",
              " 'haley',\n",
              " 'gia',\n",
              " 'kaitlyn',\n",
              " 'oakley',\n",
              " 'kailani',\n",
              " 'winter',\n",
              " 'alicia',\n",
              " 'serena',\n",
              " 'nadia',\n",
              " 'aviana',\n",
              " 'demi',\n",
              " 'jada',\n",
              " 'braelynn',\n",
              " 'dylan',\n",
              " 'ainsley',\n",
              " 'alison',\n",
              " 'camryn',\n",
              " 'avianna',\n",
              " 'bianca',\n",
              " 'skyler',\n",
              " 'scarlet',\n",
              " 'maddison',\n",
              " 'nylah',\n",
              " 'sarai',\n",
              " 'regina',\n",
              " 'dahlia',\n",
              " 'nayeli',\n",
              " 'raven',\n",
              " 'helen',\n",
              " 'adrianna',\n",
              " 'averie',\n",
              " 'skye',\n",
              " 'kelsey',\n",
              " 'tatum',\n",
              " 'kensley',\n",
              " 'maliyah',\n",
              " 'erin',\n",
              " 'viviana',\n",
              " 'jenna',\n",
              " 'anaya',\n",
              " 'carolina',\n",
              " 'shelby',\n",
              " 'sabrina',\n",
              " 'mikayla',\n",
              " 'annalise',\n",
              " 'octavia',\n",
              " 'lennon',\n",
              " 'blair',\n",
              " 'carmen',\n",
              " 'yaretzi',\n",
              " 'kennedi',\n",
              " 'mabel',\n",
              " 'zariah',\n",
              " 'kyla',\n",
              " 'christina',\n",
              " 'selah',\n",
              " 'celeste',\n",
              " 'eve',\n",
              " 'mckinley',\n",
              " 'milani',\n",
              " 'frances',\n",
              " 'jimena',\n",
              " 'kylee',\n",
              " 'leighton',\n",
              " 'katie',\n",
              " 'aitana',\n",
              " 'kayleigh',\n",
              " 'sierra',\n",
              " 'kathryn',\n",
              " 'rosemary',\n",
              " 'jolene',\n",
              " 'alondra',\n",
              " 'elisa',\n",
              " 'helena',\n",
              " 'charleigh',\n",
              " 'hallie',\n",
              " 'lainey',\n",
              " 'avah',\n",
              " 'jazlyn',\n",
              " 'kamryn',\n",
              " 'mira',\n",
              " 'cheyenne',\n",
              " 'francesca',\n",
              " 'antonella',\n",
              " 'wren',\n",
              " 'chelsea',\n",
              " 'amber',\n",
              " 'emory',\n",
              " 'lorelei',\n",
              " 'nia',\n",
              " 'abby',\n",
              " 'april',\n",
              " 'emelia',\n",
              " 'carter',\n",
              " 'aylin',\n",
              " 'cataleya',\n",
              " 'bethany',\n",
              " 'marlee',\n",
              " 'carly',\n",
              " 'kaylani',\n",
              " 'emely',\n",
              " 'liana',\n",
              " 'madelynn',\n",
              " 'cadence',\n",
              " 'matilda',\n",
              " 'sylvia',\n",
              " 'myra',\n",
              " 'fernanda',\n",
              " 'oaklyn',\n",
              " 'elianna',\n",
              " 'hattie',\n",
              " 'dayana',\n",
              " 'kendra',\n",
              " 'maisie',\n",
              " 'malaysia',\n",
              " 'kara',\n",
              " 'katelyn',\n",
              " 'maia',\n",
              " 'celine',\n",
              " 'cameron',\n",
              " 'renata',\n",
              " 'jayleen',\n",
              " 'charli',\n",
              " 'emmalyn',\n",
              " 'holly',\n",
              " 'azalea',\n",
              " 'leona',\n",
              " 'alejandra',\n",
              " 'bristol',\n",
              " 'collins',\n",
              " 'imani',\n",
              " 'meadow',\n",
              " 'alexia',\n",
              " 'edith',\n",
              " 'kaydence',\n",
              " 'leslie',\n",
              " 'lilith',\n",
              " 'kora',\n",
              " 'aisha',\n",
              " 'meredith',\n",
              " 'danna',\n",
              " 'wynter',\n",
              " 'emberly',\n",
              " 'julieta',\n",
              " 'michaela',\n",
              " 'alayah',\n",
              " 'jemma',\n",
              " 'reign',\n",
              " 'colette',\n",
              " 'kaliyah',\n",
              " 'elliott',\n",
              " 'johanna',\n",
              " 'remy',\n",
              " 'sutton',\n",
              " 'emmy',\n",
              " 'virginia',\n",
              " 'briana',\n",
              " 'oaklynn',\n",
              " 'adelina',\n",
              " 'everlee',\n",
              " 'megan',\n",
              " 'angelica',\n",
              " 'justice',\n",
              " 'mariam',\n",
              " 'khaleesi',\n",
              " 'macie',\n",
              " 'karsyn',\n",
              " 'alanna',\n",
              " 'aleah',\n",
              " 'mae',\n",
              " 'mallory',\n",
              " 'esme',\n",
              " 'skyla',\n",
              " 'madilynn',\n",
              " 'charley',\n",
              " 'allyson',\n",
              " 'hanna',\n",
              " 'shiloh',\n",
              " 'henley',\n",
              " 'macy',\n",
              " 'maryam',\n",
              " 'ivanna',\n",
              " 'ashlynn',\n",
              " 'lorelai',\n",
              " 'amora',\n",
              " 'ashlyn',\n",
              " 'sasha',\n",
              " 'baylee',\n",
              " 'beatrice',\n",
              " 'itzel',\n",
              " 'priscilla',\n",
              " 'marie',\n",
              " 'jayda',\n",
              " 'liberty',\n",
              " 'rory',\n",
              " 'alessia',\n",
              " 'alaia',\n",
              " 'janelle',\n",
              " 'kalani',\n",
              " 'gloria',\n",
              " 'sloan',\n",
              " 'dorothy',\n",
              " 'greta',\n",
              " 'julie',\n",
              " 'zahra',\n",
              " 'savanna',\n",
              " 'annabella',\n",
              " 'poppy',\n",
              " 'amalia',\n",
              " 'zaylee',\n",
              " 'cecelia',\n",
              " 'coraline',\n",
              " 'kimber',\n",
              " 'emmie',\n",
              " 'anne',\n",
              " 'karina',\n",
              " 'kassidy',\n",
              " 'kynlee',\n",
              " 'monroe',\n",
              " 'anahi',\n",
              " 'jaliyah',\n",
              " 'jazmin',\n",
              " 'maren',\n",
              " 'monica',\n",
              " 'siena',\n",
              " 'marilyn',\n",
              " 'reyna',\n",
              " 'kyra',\n",
              " 'lilian',\n",
              " 'jamie',\n",
              " 'melany',\n",
              " 'alaya',\n",
              " 'ariya',\n",
              " 'kelly',\n",
              " 'rosie',\n",
              " 'adley',\n",
              " 'dream',\n",
              " 'jaylah',\n",
              " 'laurel',\n",
              " 'jazmine',\n",
              " 'mina',\n",
              " 'karla',\n",
              " 'bailee',\n",
              " 'aubrie',\n",
              " 'katalina',\n",
              " 'melina',\n",
              " 'harlee',\n",
              " 'elliot',\n",
              " 'hayley',\n",
              " 'elaine',\n",
              " 'karen',\n",
              " 'dallas',\n",
              " 'irene',\n",
              " 'lylah',\n",
              " 'ivory',\n",
              " 'chaya',\n",
              " 'rosa',\n",
              " 'aleena',\n",
              " 'braelyn',\n",
              " 'nola',\n",
              " 'alma',\n",
              " 'leyla',\n",
              " 'pearl',\n",
              " 'addyson',\n",
              " 'roselyn',\n",
              " 'lacey',\n",
              " 'lennox',\n",
              " 'reina',\n",
              " 'aurelia',\n",
              " 'noa',\n",
              " 'janiyah',\n",
              " 'jessie',\n",
              " 'madisyn',\n",
              " 'saige',\n",
              " 'alia',\n",
              " 'tiana',\n",
              " 'astrid',\n",
              " 'cassandra',\n",
              " 'kyleigh',\n",
              " 'romina',\n",
              " 'stevie',\n",
              " 'haylee',\n",
              " 'zelda',\n",
              " 'lillie',\n",
              " 'aileen',\n",
              " 'brylee',\n",
              " 'eileen',\n",
              " 'yara',\n",
              " 'ensley',\n",
              " 'lauryn',\n",
              " 'giuliana',\n",
              " 'livia',\n",
              " 'anya',\n",
              " 'mikaela',\n",
              " 'palmer',\n",
              " 'lyra',\n",
              " 'mara',\n",
              " 'marina',\n",
              " 'kailey',\n",
              " 'liv',\n",
              " 'clementine',\n",
              " 'kenna',\n",
              " 'briar',\n",
              " 'emerie',\n",
              " 'galilea',\n",
              " 'tiffany',\n",
              " 'bonnie',\n",
              " 'elyse',\n",
              " 'cynthia',\n",
              " 'frida',\n",
              " 'kinslee',\n",
              " 'tatiana',\n",
              " 'joelle',\n",
              " 'armani',\n",
              " 'jolie',\n",
              " 'nalani',\n",
              " 'rayna',\n",
              " 'yareli',\n",
              " 'meghan',\n",
              " 'rebekah',\n",
              " 'addilynn',\n",
              " 'faye',\n",
              " 'zariyah',\n",
              " 'lea',\n",
              " 'aliza',\n",
              " 'julissa',\n",
              " 'lilyana',\n",
              " 'anika',\n",
              " 'kairi',\n",
              " 'aniya',\n",
              " 'noemi',\n",
              " 'angie',\n",
              " 'crystal',\n",
              " 'bridget',\n",
              " 'ari',\n",
              " 'davina',\n",
              " 'amelie',\n",
              " 'amirah',\n",
              " 'annika',\n",
              " 'elora',\n",
              " 'xiomara',\n",
              " 'linda',\n",
              " 'hana',\n",
              " 'laney',\n",
              " 'mercy',\n",
              " 'hadassah',\n",
              " 'madalyn',\n",
              " 'louisa',\n",
              " 'simone',\n",
              " 'kori',\n",
              " 'jillian',\n",
              " 'alena',\n",
              " 'malaya',\n",
              " 'miley',\n",
              " 'milan',\n",
              " 'sariyah',\n",
              " 'malani',\n",
              " 'clarissa',\n",
              " 'nala',\n",
              " 'princess',\n",
              " 'amani',\n",
              " 'analia',\n",
              " 'estella',\n",
              " 'milana',\n",
              " 'aya',\n",
              " 'chana',\n",
              " 'jayde',\n",
              " 'tenley',\n",
              " 'zaria',\n",
              " 'itzayana',\n",
              " 'penny',\n",
              " 'ailani',\n",
              " 'lara',\n",
              " 'aubriella',\n",
              " 'clare',\n",
              " 'lina',\n",
              " 'rhea',\n",
              " 'bria',\n",
              " 'thalia',\n",
              " 'keyla',\n",
              " 'haisley',\n",
              " 'ryann',\n",
              " 'addisyn',\n",
              " 'amaia',\n",
              " 'chanel',\n",
              " 'ellen',\n",
              " 'harmoni',\n",
              " 'aliana',\n",
              " 'tinsley',\n",
              " 'landry',\n",
              " 'paisleigh',\n",
              " 'lexie',\n",
              " 'myah',\n",
              " 'rylan',\n",
              " 'deborah',\n",
              " 'emilee',\n",
              " 'laylah',\n",
              " 'novalee',\n",
              " 'ellis',\n",
              " 'emmeline',\n",
              " 'avalynn',\n",
              " 'hadlee',\n",
              " 'legacy',\n",
              " 'braylee',\n",
              " 'elisabeth',\n",
              " 'kaylie',\n",
              " 'ansley',\n",
              " 'dior',\n",
              " 'paula',\n",
              " 'belen',\n",
              " 'corinne',\n",
              " 'maleah',\n",
              " 'martha',\n",
              " 'teresa',\n",
              " 'salma',\n",
              " 'louise',\n",
              " 'averi',\n",
              " 'lilianna',\n",
              " 'amiya',\n",
              " 'milena',\n",
              " 'royal',\n",
              " 'aubrielle',\n",
              " 'calliope',\n",
              " 'frankie',\n",
              " 'natasha',\n",
              " 'kamilah',\n",
              " 'meilani',\n",
              " 'raina',\n",
              " 'amayah',\n",
              " 'lailah',\n",
              " 'rayne',\n",
              " 'zaniyah',\n",
              " 'isabela',\n",
              " 'nathalie',\n",
              " 'miah',\n",
              " 'opal',\n",
              " 'kenia',\n",
              " 'azariah',\n",
              " 'hunter',\n",
              " 'tori',\n",
              " 'andi',\n",
              " 'keily',\n",
              " 'leanna',\n",
              " 'scarlette',\n",
              " 'jaelyn',\n",
              " 'saoirse',\n",
              " 'selene',\n",
              " 'dalary',\n",
              " 'lindsey',\n",
              " 'marianna',\n",
              " 'ramona',\n",
              " 'estelle',\n",
              " 'giovanna',\n",
              " 'holland',\n",
              " 'nancy',\n",
              " 'emmalynn',\n",
              " 'mylah',\n",
              " 'rosalee',\n",
              " 'sariah',\n",
              " 'zoie',\n",
              " 'blaire',\n",
              " 'lyanna',\n",
              " 'maxine',\n",
              " 'anais',\n",
              " 'dana',\n",
              " 'judith',\n",
              " 'kiera',\n",
              " 'jaelynn',\n",
              " 'noor',\n",
              " 'kai',\n",
              " 'adalee',\n",
              " 'oaklee',\n",
              " 'amaris',\n",
              " 'jaycee',\n",
              " 'belle',\n",
              " 'carolyn',\n",
              " 'della',\n",
              " 'karter',\n",
              " 'sky',\n",
              " 'treasure',\n",
              " 'vienna',\n",
              " 'jewel',\n",
              " 'rivka',\n",
              " 'rosalyn',\n",
              " 'alannah',\n",
              " 'ellianna',\n",
              " 'sunny',\n",
              " 'claudia',\n",
              " 'cara',\n",
              " 'hailee',\n",
              " 'estrella',\n",
              " 'harleigh',\n",
              " 'zhavia',\n",
              " 'alianna',\n",
              " 'brittany',\n",
              " 'jaylene',\n",
              " 'journi',\n",
              " 'marissa',\n",
              " 'mavis',\n",
              " 'iliana',\n",
              " 'jurnee',\n",
              " 'aislinn',\n",
              " 'alyson',\n",
              " 'elsa',\n",
              " 'kamiyah',\n",
              " 'kiana',\n",
              " 'lisa',\n",
              " 'arlette',\n",
              " 'kadence',\n",
              " 'kathleen',\n",
              " 'halle',\n",
              " 'erika',\n",
              " 'sylvie',\n",
              " 'adele',\n",
              " 'erica',\n",
              " 'veda',\n",
              " 'whitney',\n",
              " 'bexley',\n",
              " 'emmaline',\n",
              " 'guadalupe',\n",
              " 'august',\n",
              " 'brynleigh',\n",
              " 'gwen',\n",
              " 'promise',\n",
              " 'alisson',\n",
              " 'india',\n",
              " 'madalynn',\n",
              " 'paloma',\n",
              " 'patricia',\n",
              " 'samira',\n",
              " 'aliya',\n",
              " 'casey',\n",
              " 'jazlynn',\n",
              " 'paulina',\n",
              " 'dulce',\n",
              " 'kallie',\n",
              " 'perla',\n",
              " 'adrienne',\n",
              " 'alora',\n",
              " 'nataly',\n",
              " 'ayleen',\n",
              " 'christine',\n",
              " 'kaiya',\n",
              " 'ariadne',\n",
              " 'karlee',\n",
              " 'barbara',\n",
              " 'lillianna',\n",
              " 'raquel',\n",
              " 'saniyah',\n",
              " 'yamileth',\n",
              " 'arely',\n",
              " 'celia',\n",
              " 'heavenly',\n",
              " 'kaylin',\n",
              " 'marisol',\n",
              " 'marleigh',\n",
              " 'avalyn',\n",
              " 'berkley',\n",
              " 'kataleya',\n",
              " 'zainab',\n",
              " 'dani',\n",
              " 'egypt',\n",
              " 'joyce',\n",
              " 'kenley',\n",
              " 'annabel',\n",
              " 'kaelyn',\n",
              " 'etta',\n",
              " 'hadleigh',\n",
              " 'joselyn',\n",
              " 'luella',\n",
              " 'jaylee',\n",
              " 'zola',\n",
              " 'alisha',\n",
              " 'ezra',\n",
              " 'queen',\n",
              " 'amia',\n",
              " 'annalee',\n",
              " 'bellamy',\n",
              " 'paola',\n",
              " 'tinley',\n",
              " 'violeta',\n",
              " 'jenesis',\n",
              " 'arden',\n",
              " 'giana',\n",
              " 'wendy',\n",
              " 'ellison',\n",
              " 'florence',\n",
              " 'margo',\n",
              " 'naya',\n",
              " 'robin',\n",
              " 'sandra',\n",
              " 'scout',\n",
              " 'waverly',\n",
              " 'janessa',\n",
              " 'jayden',\n",
              " 'micah',\n",
              " 'novah',\n",
              " 'zora',\n",
              " 'ann',\n",
              " 'jana',\n",
              " 'taliyah',\n",
              " 'vada',\n",
              " 'giavanna',\n",
              " 'ingrid',\n",
              " 'valery',\n",
              " 'azaria',\n",
              " 'emmarie',\n",
              " 'esperanza',\n",
              " 'kailyn',\n",
              " 'aiyana',\n",
              " 'keilani',\n",
              " 'austyn',\n",
              " 'whitley',\n",
              " 'elina',\n",
              " 'kimora',\n",
              " 'maliah',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "names_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUGS4Rf6OFgZ"
      },
      "source": [
        "# N-Gram\n",
        "- Start with bi-gram (2-gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BUchO_cFOE6O"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# bigram_dict = {}\n",
        "bigram_dict = defaultdict(int) # If key is not in the defaultdict, it automatically assign key and empty value (int=0, list=[])\n",
        "unigram_dict = defaultdict(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53aB-trqviem"
      },
      "source": [
        "# RNN\n",
        "- $h_t = \\tanh(\\textbf{W}_{hh}h_{t-1} + \\textbf{W}_{xh}x_t + b) $\n",
        "  - $\\textbf{W}$: Weight Matrix\n",
        "  - $b$: bias\n",
        "  - $x_t$: input vector of time step $t$\n",
        "  - $h_t$: hidden state (and also output) of time step $t$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7bIx3l-6kmm",
        "outputId": "1b5a1096-3e49-44cf-e701-4cad13ef5969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.0554,  0.1778, -0.2303])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "sequence_length = 7\n",
        "input_dim, hidden_dim = 3, 5\n",
        "weight_hh = nn.Linear(hidden_dim, hidden_dim)\n",
        "weight_xh = nn.Linear(input_dim, hidden_dim)\n",
        "h0 = torch.zeros(hidden_dim)\n",
        "x = torch.randn([sequence_length, input_dim])\n",
        "t = 0\n",
        "x_t = x[t]\n",
        "x[t]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGZUlBcN9nuj",
        "outputId": "f0b8aa20-a77a-4f8d-d1b0-2fb34763c063"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3031,  0.4942, -0.3826, -0.1671, -0.0307], grad_fn=<TanhBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "h_t = torch.tanh(weight_hh(h0)+weight_xh(x_t))\n",
        "h_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgKeelj0_MFn",
        "outputId": "fceee9b8-256e-4225-eae3-69a147a2b905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([ 1.0554,  0.1778, -0.2303])\n",
            "h: tensor([-0.3031,  0.4942, -0.3826, -0.1671, -0.0307], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-0.3918,  0.5433,  0.3356])\n",
            "h: tensor([ 0.2949,  0.2907,  0.5566, -0.6004, -0.4537], grad_fn=<TanhBackward0>)\n",
            "x: tensor([1.5091, 2.0820, 1.7067])\n",
            "h: tensor([-0.0504, -0.8319,  0.6891, -0.0811, -0.9549], grad_fn=<TanhBackward0>)\n",
            "x: tensor([ 2.3804, -1.1256, -0.3170])\n",
            "h: tensor([-0.9035,  0.7153, -0.9110,  0.4101,  0.4610], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-1.0925,  0.8058,  0.3276])\n",
            "h: tensor([ 0.5157,  0.1567,  0.7691, -0.8519, -0.4661], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-0.7607, -1.5991,  0.0185])\n",
            "h: tensor([-0.6471,  0.9578, -0.5932, -0.2097,  0.4347], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-0.7504,  0.1854,  0.6211])\n",
            "h: tensor([ 0.2194,  0.3107,  0.5832, -0.7386, -0.3476], grad_fn=<TanhBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3031,  0.4942, -0.3826, -0.1671, -0.0307],\n",
              "        [ 0.2949,  0.2907,  0.5566, -0.6004, -0.4537],\n",
              "        [-0.0504, -0.8319,  0.6891, -0.0811, -0.9549],\n",
              "        [-0.9035,  0.7153, -0.9110,  0.4101,  0.4610],\n",
              "        [ 0.5157,  0.1567,  0.7691, -0.8519, -0.4661],\n",
              "        [-0.6471,  0.9578, -0.5932, -0.2097,  0.4347],\n",
              "        [ 0.2194,  0.3107,  0.5832, -0.7386, -0.3476]],\n",
              "       grad_fn=<StackBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def run_rnn_cell(weight_hh, weight_xh, prev_h, x_t):\n",
        "  return torch.tanh(weight_hh(prev_h)+weight_xh(x_t))\n",
        "\n",
        "output = []\n",
        "prev_h = h0\n",
        "for i in range(len(x)):\n",
        "  print(f\"x: {x[i]}\")\n",
        "  h = run_rnn_cell(weight_hh, weight_xh, prev_h, x[i])\n",
        "  prev_h = h\n",
        "  output.append(h)\n",
        "  print(f\"h: {h}\")\n",
        "\n",
        "output = torch.stack(output)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiaB2EqJBJOu",
        "outputId": "7f60f0ed-0125-41f8-d4da-73f882d096ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "names_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC_G9VPzDCzE",
        "outputId": "6e8c8301-1942-4c3f-d213-ce4df6cef020"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196113"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "entire_chars = []\n",
        "\n",
        "for name in names_list:\n",
        "  for char in name:\n",
        "    entire_chars.append(char)\n",
        "\n",
        "len(entire_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJq1DgIBDs6_",
        "outputId": "2a843ba5-18dc-4fc2-910e-0b02c2c99e62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0,\n",
              " 'b': 1,\n",
              " 'c': 2,\n",
              " 'd': 3,\n",
              " 'e': 4,\n",
              " 'f': 5,\n",
              " 'g': 6,\n",
              " 'h': 7,\n",
              " 'i': 8,\n",
              " 'j': 9,\n",
              " 'k': 10,\n",
              " 'l': 11,\n",
              " 'm': 12,\n",
              " 'n': 13,\n",
              " 'o': 14,\n",
              " 'p': 15,\n",
              " 'q': 16,\n",
              " 'r': 17,\n",
              " 's': 18,\n",
              " 't': 19,\n",
              " 'u': 20,\n",
              " 'v': 21,\n",
              " 'w': 22,\n",
              " 'x': 23,\n",
              " 'y': 24,\n",
              " 'z': 25}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "set(entire_chars)\n",
        "vocab = list(set(entire_chars))\n",
        "vocab.sort()\n",
        "\n",
        "char2idx = {char: i for i, char in enumerate(vocab)}\n",
        "char2idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYFTB9TR5WPE"
      },
      "source": [
        "## Define Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnX2HlaT5WPE",
        "outputId": "06945dab-92dd-4ebf-cf80-69e58524f50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<start>', '<end>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "class NameSet:\n",
        "  def __init__(self, txt_fn):\n",
        "    txt_string = read_txt(txt_fn)\n",
        "    names_list = [x.replace(\"\\n\", '') for x in txt_string]\n",
        "    self.data = names_list\n",
        "    entire_chars = []\n",
        "    for name in names_list:\n",
        "      for char in name:\n",
        "        entire_chars.append(char)\n",
        "\n",
        "    self.vocab = list(set(entire_chars))\n",
        "    self.vocab.sort()\n",
        "\n",
        "    special_tokens = ['<pad>', '<start>', '<end>']\n",
        "    self.vocab = special_tokens+self.vocab\n",
        "    self.char2idx = {char: i for i, char in enumerate(self.vocab)}\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):\n",
        "    name_string = self.data[idx]\n",
        "    name_in_idx = [self.char2idx[char] for char in name_string]\n",
        "    name_in_idx = [self.char2idx['<start>']] + name_in_idx + [self.char2idx['<end>']]\n",
        "\n",
        "    model_input = name_in_idx[:-1]\n",
        "    target_output = name_in_idx[1:]\n",
        "    return model_input, target_output\n",
        "\n",
        "\n",
        "dataset = NameSet('names.txt')\n",
        "dataset.data[0]\n",
        "len(dataset)\n",
        "print(dataset.vocab)\n",
        "dataset[0]\n",
        "len(dataset.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cGwvuZOBtVJ",
        "outputId": "05bb6bde-7b90-45bc-d2b6-6ef521677adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e 7\n",
            "m 15\n",
            "m 15\n",
            "a 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 15, 15, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "name = 'emma' #. 4, 12, 12, 0\n",
        "char2idx = dataset.char2idx\n",
        "\n",
        "for char in name:\n",
        "  print(char, char2idx[char])\n",
        "\n",
        "[char2idx[char] for char in name] # tokenizing string to tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Se8szyU5WPE"
      },
      "source": [
        "## Define the model\n",
        "- Model Architecture\n",
        "  - Embedding Layer: Convert character to vector\n",
        "  - RNN Layer: Process the input sequence\n",
        "  - Linear Layer: Output the logits for the probability of the next character\n",
        "  - Softmax Layer: Convert the logits to probability\n",
        "\n",
        "- Forward Pass:\n",
        "  - Assume the input is a sequence of characters\n",
        "    - We call it \"teacher-forcing\". We feed the target output to the model as the input\n",
        "  - Flow: Embedding -> RNN -> Linear -> Softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3o8ySWcu5WPE",
        "outputId": "24f5214c-31a5-4b15-8ebe-a286e0a5d132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1,  7, 15, 15,  3])\n",
            "torch.Size([5, 29])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0435, 0.0353, 0.0337, 0.0384, 0.0331, 0.0350, 0.0284, 0.0374, 0.0393,\n",
              "         0.0368, 0.0298, 0.0337, 0.0297, 0.0372, 0.0387, 0.0279, 0.0380, 0.0412,\n",
              "         0.0285, 0.0354, 0.0322, 0.0392, 0.0364, 0.0304, 0.0379, 0.0285, 0.0330,\n",
              "         0.0280, 0.0333],\n",
              "        [0.0409, 0.0335, 0.0454, 0.0313, 0.0343, 0.0304, 0.0235, 0.0459, 0.0394,\n",
              "         0.0379, 0.0317, 0.0303, 0.0278, 0.0381, 0.0411, 0.0296, 0.0313, 0.0340,\n",
              "         0.0295, 0.0349, 0.0362, 0.0388, 0.0361, 0.0317, 0.0393, 0.0291, 0.0355,\n",
              "         0.0274, 0.0350],\n",
              "        [0.0457, 0.0299, 0.0402, 0.0285, 0.0329, 0.0328, 0.0300, 0.0430, 0.0400,\n",
              "         0.0332, 0.0393, 0.0312, 0.0261, 0.0303, 0.0379, 0.0269, 0.0249, 0.0358,\n",
              "         0.0336, 0.0416, 0.0349, 0.0447, 0.0397, 0.0319, 0.0416, 0.0315, 0.0362,\n",
              "         0.0232, 0.0323],\n",
              "        [0.0463, 0.0287, 0.0393, 0.0277, 0.0333, 0.0331, 0.0328, 0.0418, 0.0387,\n",
              "         0.0315, 0.0428, 0.0310, 0.0261, 0.0279, 0.0384, 0.0258, 0.0220, 0.0348,\n",
              "         0.0362, 0.0456, 0.0346, 0.0475, 0.0397, 0.0313, 0.0450, 0.0328, 0.0350,\n",
              "         0.0210, 0.0292],\n",
              "        [0.0381, 0.0292, 0.0346, 0.0330, 0.0475, 0.0322, 0.0356, 0.0431, 0.0290,\n",
              "         0.0312, 0.0346, 0.0311, 0.0291, 0.0251, 0.0378, 0.0313, 0.0215, 0.0264,\n",
              "         0.0361, 0.0469, 0.0378, 0.0459, 0.0365, 0.0320, 0.0430, 0.0381, 0.0382,\n",
              "         0.0239, 0.0313]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size,embedding_dim=16):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb = nn.Embedding(embedding_dim=embedding_dim,num_embeddings=self.vocab_size)\n",
        "    self.rnn = nn.GRU(input_size=embedding_dim,hidden_size=2*embedding_dim)\n",
        "\n",
        "    self.proj = nn.Linear(in_features= 2*embedding_dim, out_features= vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.emb(x)\n",
        "    x,last_hidden = self.rnn(x)\n",
        "    self.proj\n",
        "    x = self.proj(x)\n",
        "    x = torch.softmax(x, dim=1)\n",
        "    return x\n",
        "vocab_size = len(dataset.vocab)\n",
        "model=LanguageModel(vocab_size)\n",
        "model.emb.weight\n",
        "\n",
        "x,y = dataset[0]\n",
        "x = torch.tensor(x)\n",
        "print(x)\n",
        "print(model(x).shape)\n",
        "model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcYnHAXM9KTQ"
      },
      "source": [
        "## Define Collate Function\n",
        "- As the input is a list of arbitrary length, we need to pad them to the same length\n",
        "- You can feed collate function to the DataLoader\n",
        "  - ```dataloader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MFX4Cs8g9KTQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "fbf5b98a-a4c1-41a9-f4b0-923d6b448923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 7, 15, 15, 3] 4\n",
            "[1, 17, 14, 11, 24, 11, 3] 2\n",
            "[1, 3, 24, 3] 5\n",
            "[1, 11, 21, 3, 4, 7, 14, 14, 3] 0\n",
            "[4, 2, 5, 0]\n",
            "9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected sequence of length 5 at dim 1 (got 7)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ff9a168c542e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0;31m# [dataset[idx] for idx in [0,1,2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-ff9a168c542e>\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(raw_batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 5 at dim 1 (got 7)"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate(raw_batch):\n",
        "  # for x,t in raw_batch\n",
        "  x = [item[0] for item in raw_batch]\n",
        "  y = [item[1] for item in raw_batch]\n",
        "\n",
        "  length_name = max([len(name) for name in x])\n",
        "  num_pad = [length_name - len(name) for name in x]\n",
        "  # num_pad[]\n",
        "  # for name in x :\n",
        "  #   num_pad.append(length_name - len(name))\n",
        "  pad_x = []\n",
        "  for name, pad in zip(x, num_pad) :\n",
        "    print(name, pad)\n",
        "    paded_name = name + [0] * pad\n",
        "\n",
        "  print(num_pad)\n",
        "  print(length_name)\n",
        "  return torch.tensor(x)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size = 4, collate_fn = collate)\n",
        "\n",
        "for batch in dataloader:\n",
        "  # [dataset[idx] for idx in [0,1,2]]\n",
        "  print(batch)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC-7TBKU9KTQ"
      },
      "source": [
        "## Define the split function\n",
        "- Split the dataset into training and validation set\n",
        "- You can use ```torch.utils.data.random_split``` function\n",
        "  - ```train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_S848Uv9KTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcvOGUiE5WPE"
      },
      "source": [
        "## Define Training Loop\n",
        "- Define the loss function\n",
        "  - Negative Log Likelihood Loss: Consider that we are using padding tokens as well\n",
        "- Define the loss using the loss function\n",
        "- Save the loss value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwtEtIj09KTQ",
        "outputId": "128f198f-a70b-4531-8946-3b0642d68e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 7, 15, 15,  3,  2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3725, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "x, y = dataset[0]\n",
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "prediction = model(x)\n",
        "print(y)\n",
        "\n",
        "prediction\n",
        "\n",
        "prob_of_correct_char = prediction[torch.arange(len(y)),y]\n",
        "prob_of_correct_char\n",
        "nll = -torch.log(prob_of_correct_char)\n",
        "nll.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq5SklLf9KTR"
      },
      "source": [
        "## Define the Inference\n",
        "- Unlike the training, we don't have the target output in the inference\n",
        "- We need to feed the model with the previous character and get the next character as the output\n",
        "  - We have to \"sample\" the next character from the model.\n",
        "    - For this, we can use the ```torch.multinomial``` function\n",
        "    - ```torch.multinomial(logits, num_samples=1)```\n",
        "    - This function will sample the next character from the logits\n",
        "    - We can use this function to sample the next character in the inference loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvDnSV5o9KTR"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}