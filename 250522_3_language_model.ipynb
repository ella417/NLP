{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ella417/NLP/blob/main/250522_3_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8vQzOle74-cJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def read_txt(txt_path):\n",
        "  with open(txt_path, 'r') as f:\n",
        "    txt_string = f.readlines()\n",
        "  return txt_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXbsjcoKIC93"
      },
      "source": [
        "# Language modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1EwAtYF4-cJ",
        "outputId": "833dd7dd-98f3-4db0-ee2a-3ecaa508b526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-22 05:16:48--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-22 05:16:48 (9.50 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xbdOy7ia4-cK"
      },
      "outputs": [],
      "source": [
        "txt_string = read_txt('names.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSIJEvdSN3aO",
        "outputId": "83cdeb27-a32b-46b3-a871-6f5316aa21df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "names_list = [x.replace('\\n', '') for x in txt_string]\n",
        "len(names_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tijegdHb6RAj"
      },
      "outputs": [],
      "source": [
        "# names_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUGS4Rf6OFgZ"
      },
      "source": [
        "# N-Gram\n",
        "- Start with bi-gram (2-gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BUchO_cFOE6O"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# bigram_dict = {}\n",
        "bigram_dict = defaultdict(int) # If key is not in the defaultdict, it automatically assign key and empty value (int=0, list=[])\n",
        "unigram_dict = defaultdict(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53aB-trqviem"
      },
      "source": [
        "# RNN\n",
        "- $h_t = \\tanh(\\textbf{W}_{hh}h_{t-1} + \\textbf{W}_{xh}x_t + b) $\n",
        "  - $\\textbf{W}$: Weight Matrix\n",
        "  - $b$: bias\n",
        "  - $x_t$: input vector of time step $t$\n",
        "  - $h_t$: hidden state (and also output) of time step $t$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7bIx3l-6kmm",
        "outputId": "fae187d7-25aa-455b-9d49-fc1969c409a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.0554,  0.1778, -0.2303])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "sequence_length = 7\n",
        "input_dim, hidden_dim = 3, 5\n",
        "weight_hh = nn.Linear(hidden_dim, hidden_dim)\n",
        "weight_xh = nn.Linear(input_dim, hidden_dim)\n",
        "h0 = torch.zeros(hidden_dim)\n",
        "x = torch.randn([sequence_length, input_dim])\n",
        "t = 0\n",
        "x_t = x[t]\n",
        "x[t]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGZUlBcN9nuj",
        "outputId": "57b5f9d6-8b5d-4405-dde3-129c7968955f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3031,  0.4942, -0.3826, -0.1671, -0.0307], grad_fn=<TanhBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "h_t = torch.tanh(weight_hh(h0)+weight_xh(x_t))\n",
        "h_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgKeelj0_MFn",
        "outputId": "47158169-5914-459e-e7ac-cc6510c396ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([ 1.0554,  0.1778, -0.2303])\n",
            "h: tensor([-0.3031,  0.4942, -0.3826, -0.1671, -0.0307], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-0.3918,  0.5433,  0.3356])\n",
            "h: tensor([ 0.2949,  0.2907,  0.5566, -0.6004, -0.4537], grad_fn=<TanhBackward0>)\n",
            "x: tensor([1.5091, 2.0820, 1.7067])\n",
            "h: tensor([-0.0504, -0.8319,  0.6891, -0.0811, -0.9549], grad_fn=<TanhBackward0>)\n",
            "x: tensor([ 2.3804, -1.1256, -0.3170])\n",
            "h: tensor([-0.9035,  0.7153, -0.9110,  0.4101,  0.4610], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-1.0925,  0.8058,  0.3276])\n",
            "h: tensor([ 0.5157,  0.1567,  0.7691, -0.8519, -0.4661], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-0.7607, -1.5991,  0.0185])\n",
            "h: tensor([-0.6471,  0.9578, -0.5932, -0.2097,  0.4347], grad_fn=<TanhBackward0>)\n",
            "x: tensor([-0.7504,  0.1854,  0.6211])\n",
            "h: tensor([ 0.2194,  0.3107,  0.5832, -0.7386, -0.3476], grad_fn=<TanhBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3031,  0.4942, -0.3826, -0.1671, -0.0307],\n",
              "        [ 0.2949,  0.2907,  0.5566, -0.6004, -0.4537],\n",
              "        [-0.0504, -0.8319,  0.6891, -0.0811, -0.9549],\n",
              "        [-0.9035,  0.7153, -0.9110,  0.4101,  0.4610],\n",
              "        [ 0.5157,  0.1567,  0.7691, -0.8519, -0.4661],\n",
              "        [-0.6471,  0.9578, -0.5932, -0.2097,  0.4347],\n",
              "        [ 0.2194,  0.3107,  0.5832, -0.7386, -0.3476]],\n",
              "       grad_fn=<StackBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def run_rnn_cell(weight_hh, weight_xh, prev_h, x_t):\n",
        "  return torch.tanh(weight_hh(prev_h)+weight_xh(x_t))\n",
        "\n",
        "output = []\n",
        "prev_h = h0\n",
        "for i in range(len(x)):\n",
        "  print(f\"x: {x[i]}\")\n",
        "  h = run_rnn_cell(weight_hh, weight_xh, prev_h, x[i])\n",
        "  prev_h = h\n",
        "  output.append(h)\n",
        "  print(f\"h: {h}\")\n",
        "\n",
        "output = torch.stack(output)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiaB2EqJBJOu",
        "outputId": "1a8f6888-c10a-49e4-c76f-bccbe15f7881"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "names_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC_G9VPzDCzE",
        "outputId": "34121e2d-7b0d-4fdd-ba91-2cf3fa13e2c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196113"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "entire_chars = []\n",
        "\n",
        "for name in names_list:\n",
        "  for char in name:\n",
        "    entire_chars.append(char)\n",
        "\n",
        "len(entire_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJq1DgIBDs6_",
        "outputId": "b89022a1-a7d3-4a51-fc51-fa7d28953a22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0,\n",
              " 'b': 1,\n",
              " 'c': 2,\n",
              " 'd': 3,\n",
              " 'e': 4,\n",
              " 'f': 5,\n",
              " 'g': 6,\n",
              " 'h': 7,\n",
              " 'i': 8,\n",
              " 'j': 9,\n",
              " 'k': 10,\n",
              " 'l': 11,\n",
              " 'm': 12,\n",
              " 'n': 13,\n",
              " 'o': 14,\n",
              " 'p': 15,\n",
              " 'q': 16,\n",
              " 'r': 17,\n",
              " 's': 18,\n",
              " 't': 19,\n",
              " 'u': 20,\n",
              " 'v': 21,\n",
              " 'w': 22,\n",
              " 'x': 23,\n",
              " 'y': 24,\n",
              " 'z': 25}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "set(entire_chars)\n",
        "vocab = list(set(entire_chars))\n",
        "vocab.sort()\n",
        "\n",
        "char2idx = {char: i for i, char in enumerate(vocab)}\n",
        "char2idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYFTB9TR5WPE"
      },
      "source": [
        "## Define Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnX2HlaT5WPE",
        "outputId": "5bd5f4ce-1c79-4140-a582-b3a9c610916d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<start>', '<end>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "class NameSet:\n",
        "  def __init__(self, txt_fn):\n",
        "    txt_string = read_txt(txt_fn)\n",
        "    names_list = [x.replace(\"\\n\", '') for x in txt_string]\n",
        "    self.data = names_list\n",
        "    entire_chars = []\n",
        "    for name in names_list:\n",
        "      for char in name:\n",
        "        entire_chars.append(char)\n",
        "\n",
        "    self.vocab = list(set(entire_chars))\n",
        "    self.vocab.sort()\n",
        "\n",
        "    special_tokens = ['<pad>', '<start>', '<end>']\n",
        "    self.vocab = special_tokens+self.vocab\n",
        "    self.char2idx = {char: i for i, char in enumerate(self.vocab)}\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):\n",
        "    name_string = self.data[idx]\n",
        "    name_in_idx = [self.char2idx[char] for char in name_string]\n",
        "    name_in_idx = [self.char2idx['<start>']] + name_in_idx + [self.char2idx['<end>']]\n",
        "\n",
        "    model_input = name_in_idx[:-1]\n",
        "    target_output = name_in_idx[1:]\n",
        "    return model_input, target_output\n",
        "\n",
        "\n",
        "dataset = NameSet('names.txt')\n",
        "dataset.data[0]\n",
        "len(dataset)\n",
        "print(dataset.vocab)\n",
        "dataset[0]\n",
        "len(dataset.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cGwvuZOBtVJ",
        "outputId": "86a4c247-668c-4def-be1c-e071081bd915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e 7\n",
            "m 15\n",
            "m 15\n",
            "a 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 15, 15, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "name = 'emma' #. 4, 12, 12, 0\n",
        "char2idx = dataset.char2idx\n",
        "\n",
        "for char in name:\n",
        "  print(char, char2idx[char])\n",
        "\n",
        "[char2idx[char] for char in name] # tokenizing string to tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Se8szyU5WPE"
      },
      "source": [
        "## Define the model\n",
        "- Model Architecture\n",
        "  - Embedding Layer: Convert character to vector\n",
        "  - RNN Layer: Process the input sequence\n",
        "  - Linear Layer: Output the logits for the probability of the next character\n",
        "  - Softmax Layer: Convert the logits to probability\n",
        "\n",
        "- Forward Pass:\n",
        "  - Assume the input is a sequence of characters\n",
        "    - We call it \"teacher-forcing\". We feed the target output to the model as the input\n",
        "  - Flow: Embedding -> RNN -> Linear -> Softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3o8ySWcu5WPE",
        "outputId": "b1889d76-5bbf-4a2f-d9d9-bdbc12256e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1,  7, 15, 15,  3])\n",
            "torch.Size([5, 29])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0435, 0.0353, 0.0337, 0.0384, 0.0331, 0.0350, 0.0284, 0.0374, 0.0393,\n",
              "         0.0368, 0.0298, 0.0337, 0.0297, 0.0372, 0.0387, 0.0279, 0.0380, 0.0412,\n",
              "         0.0285, 0.0354, 0.0322, 0.0392, 0.0364, 0.0304, 0.0379, 0.0285, 0.0330,\n",
              "         0.0280, 0.0333],\n",
              "        [0.0409, 0.0335, 0.0454, 0.0313, 0.0343, 0.0304, 0.0235, 0.0459, 0.0394,\n",
              "         0.0379, 0.0317, 0.0303, 0.0278, 0.0381, 0.0411, 0.0296, 0.0313, 0.0340,\n",
              "         0.0295, 0.0349, 0.0362, 0.0388, 0.0361, 0.0317, 0.0393, 0.0291, 0.0355,\n",
              "         0.0274, 0.0350],\n",
              "        [0.0457, 0.0299, 0.0402, 0.0285, 0.0329, 0.0328, 0.0300, 0.0430, 0.0400,\n",
              "         0.0332, 0.0393, 0.0312, 0.0261, 0.0303, 0.0379, 0.0269, 0.0249, 0.0358,\n",
              "         0.0336, 0.0416, 0.0349, 0.0447, 0.0397, 0.0319, 0.0416, 0.0315, 0.0362,\n",
              "         0.0232, 0.0323],\n",
              "        [0.0463, 0.0287, 0.0393, 0.0277, 0.0333, 0.0331, 0.0328, 0.0418, 0.0387,\n",
              "         0.0315, 0.0428, 0.0310, 0.0261, 0.0279, 0.0384, 0.0258, 0.0220, 0.0348,\n",
              "         0.0362, 0.0456, 0.0346, 0.0475, 0.0397, 0.0313, 0.0450, 0.0328, 0.0350,\n",
              "         0.0210, 0.0292],\n",
              "        [0.0381, 0.0292, 0.0346, 0.0330, 0.0475, 0.0322, 0.0356, 0.0431, 0.0290,\n",
              "         0.0312, 0.0346, 0.0311, 0.0291, 0.0251, 0.0378, 0.0313, 0.0215, 0.0264,\n",
              "         0.0361, 0.0469, 0.0378, 0.0459, 0.0365, 0.0320, 0.0430, 0.0381, 0.0382,\n",
              "         0.0239, 0.0313]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size,embedding_dim=16):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb = nn.Embedding(embedding_dim=embedding_dim,num_embeddings=self.vocab_size)\n",
        "    self.rnn = nn.GRU(input_size=embedding_dim,hidden_size=2*embedding_dim)\n",
        "\n",
        "    self.proj = nn.Linear(in_features= 2*embedding_dim, out_features= vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.emb(x)\n",
        "    x,last_hidden = self.rnn(x)\n",
        "    self.proj\n",
        "    x = self.proj(x)\n",
        "    x = torch.softmax(x, dim=1)\n",
        "    return x\n",
        "vocab_size = len(dataset.vocab)\n",
        "model=LanguageModel(vocab_size)\n",
        "model.emb.weight\n",
        "\n",
        "x,y = dataset[0]\n",
        "x = torch.tensor(x)\n",
        "print(x)\n",
        "print(model(x).shape)\n",
        "model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcYnHAXM9KTQ"
      },
      "source": [
        "## Define Collate Function\n",
        "- As the input is a list of arbitrary length, we need to pad them to the same length\n",
        "- You can feed collate function to the DataLoader\n",
        "  - ```dataloader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MFX4Cs8g9KTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8674ce19-6528-449b-846f-bfce28f27c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[ 1,  7, 15, 15,  3,  0,  0,  0,  0,  0],\n",
            "        [ 1, 17, 14, 11, 24, 11,  3,  0,  0,  0],\n",
            "        [ 1,  3, 24,  3,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 11, 21,  3,  4,  7, 14, 14,  3,  0],\n",
            "        [ 1, 21, 17, 18, 10, 11,  3,  0,  0,  0],\n",
            "        [ 1,  5, 10,  3, 20, 14, 17, 22, 22,  7],\n",
            "        [ 1, 15, 11,  3,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  3, 15,  7, 14, 11,  3,  0,  0,  0]]), tensor([[ 7, 15, 15,  3,  2,  0,  0,  0,  0,  0],\n",
            "        [17, 14, 11, 24, 11,  3,  2,  0,  0,  0],\n",
            "        [ 3, 24,  3,  2,  0,  0,  0,  0,  0,  0],\n",
            "        [11, 21,  3,  4,  7, 14, 14,  3,  2,  0],\n",
            "        [21, 17, 18, 10, 11,  3,  2,  0,  0,  0],\n",
            "        [ 5, 10,  3, 20, 14, 17, 22, 22,  7,  2],\n",
            "        [15, 11,  3,  2,  0,  0,  0,  0,  0,  0],\n",
            "        [ 3, 15,  7, 14, 11,  3,  2,  0,  0,  0]]))\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate(raw_batch):\n",
        "  # for x,t in raw_batch\n",
        "  x = [item[0] for item in raw_batch]\n",
        "  y = [item[1] for item in raw_batch]\n",
        "\n",
        "  length_name = max([len(name) for name in x])\n",
        "  num_pad = [length_name - len(name) for name in x]\n",
        "  # num_pad[]\n",
        "  # for name in x :\n",
        "  #   num_pad.append(length_name - len(name))\n",
        "  pad_x = []\n",
        "  pad_y = []\n",
        "  for name, name_y, pad in zip(x, y, num_pad) :\n",
        "    # print(name, name_y, pad)\n",
        "    paded_name = name + [0] * pad\n",
        "    paded_name_y = name_y + [0] * pad\n",
        "\n",
        "    pad_x.append(paded_name)\n",
        "    pad_y.append(paded_name_y)\n",
        "\n",
        "#  pad_x = torch.zeros(len(raw_batch), length_name, dtype=torch.long)\n",
        "#  for i, name in enumerate(x):\n",
        "#    pad_x[i, len(name)] = torch.tensor(name)\n",
        "\n",
        "  x = pad_x\n",
        "  y = pad_y\n",
        "\n",
        "  # print(num_pad)\n",
        "  # print(length_name)\n",
        "  return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size = 8, collate_fn = collate)\n",
        "\n",
        "for batch in dataloader:\n",
        "  # [dataset[idx] for idx in [0,1,2]]\n",
        "  print(batch)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC-7TBKU9KTQ"
      },
      "source": [
        "## Define the split function\n",
        "- Split the dataset into training and validation set\n",
        "- You can use ```torch.utils.data.random_split``` function\n",
        "  - ```train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1_S848Uv9KTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f852a7-fc55-485a-8b59-33a065f829d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28829 3204\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(dataset) * 0.9)\n",
        "val_size = len(dataset) - train_size\n",
        "print(train_size, val_size)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcvOGUiE5WPE"
      },
      "source": [
        "## Define Training Loop\n",
        "- Define the loss function\n",
        "  - Negative Log Likelihood Loss: Consider that we are using padding tokens as well\n",
        "- Define the loss using the loss function\n",
        "- Save the loss value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "UwtEtIj09KTQ",
        "outputId": "d3beaba8-d575-4153-b582-2b0d17796790",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 7, 15, 15,  3,  2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.4748, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "x, y = dataset[0]\n",
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "prediction = model(x)\n",
        "print(y)\n",
        "\n",
        "prediction\n",
        "\n",
        "prob_of_correct_char = prediction[torch.arange(len(y)),y]\n",
        "prob_of_correct_char\n",
        "nll = -torch.log(prob_of_correct_char)\n",
        "nll.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 8, collate_fn = collate, shuffle = True)\n",
        "batch = next(iter(train_loader))\n",
        "# iterater\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk4OApRsdNym",
        "outputId": "21312ed8-3fbf-4b81-f988-2cbb8ad1a61c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1,  8,  3, 14, 14, 27, 16,  0,  0,  0],\n",
              "         [ 1, 20,  7, 28, 25,  3, 16,  0,  0,  0],\n",
              "         [ 1, 14,  7, 26, 11, 16,  9, 22, 17, 16],\n",
              "         [ 1,  3, 14, 18, 10, 17, 16, 28, 17,  0],\n",
              "         [ 1, 12,  3, 11, 17, 16, 16,  3,  0,  0],\n",
              "         [ 1, 12,  3, 16, 11, 27, 14,  3,  0,  0],\n",
              "         [ 1, 22,  3, 20,  7, 13,  0,  0,  0,  0],\n",
              "         [ 1,  3, 24,  7, 20, 27,  3, 16,  3,  0]]),\n",
              " tensor([[ 8,  3, 14, 14, 27, 16,  2,  0,  0,  0],\n",
              "         [20,  7, 28, 25,  3, 16,  2,  0,  0,  0],\n",
              "         [14,  7, 26, 11, 16,  9, 22, 17, 16,  2],\n",
              "         [ 3, 14, 18, 10, 17, 16, 28, 17,  2,  0],\n",
              "         [12,  3, 11, 17, 16, 16,  3,  2,  0,  0],\n",
              "         [12,  3, 16, 11, 27, 14,  3,  2,  0,  0],\n",
              "         [22,  3, 20,  7, 13,  2,  0,  0,  0,  0],\n",
              "         [ 3, 24,  7, 20, 27,  3, 16,  3,  2,  0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq5SklLf9KTR"
      },
      "source": [
        "## Define the Inference\n",
        "- Unlike the training, we don't have the target output in the inference\n",
        "- We need to feed the model with the previous character and get the next character as the output\n",
        "  - We have to \"sample\" the next character from the model.\n",
        "    - For this, we can use the ```torch.multinomial``` function\n",
        "    - ```torch.multinomial(logits, num_samples=1)```\n",
        "    - This function will sample the next character from the logits\n",
        "    - We can use this function to sample the next character in the inference loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvDnSV5o9KTR"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}