{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ella417/NLP/blob/main/NLP_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38583105",
      "metadata": {
        "id": "38583105"
      },
      "source": [
        "# Assignment 4: Attention\n",
        "- In this assignment, you have to implement the attention mechanism for the machine translation task.\n",
        "\n",
        "- Problems\n",
        "  - 1. Implement Dot Product Attention (10 pts)\n",
        "  - 2. Implement Attention in Batch (12 pts)\n",
        "  - 3. Implement Seq2seq with Attention (14 pts)\n",
        "  - 4. Implement Transformer-like Attention (14 pts)\n",
        "    - Self-attention\n",
        "    - Cross-attention\n",
        "\n",
        "- Submission\n",
        "  - You have to copy and paste your answer to ``NLP_Assignment_4.py`` file and submit it to Cyber Campus\n",
        "    - Submssion file name has to be ``NLP_Assignment_4_{your_student_id}.py``\n",
        "    - { } character is just placeholder, so don't inlcude it in your filename.\n",
        "\n",
        "- CAUTION:\n",
        "  - You have to implement most of the functions with **vectorized matrix multiplication**, not with for loop\n",
        "\n",
        "\n",
        "\n",
        "- If you find any error, please do not hesitate to report or make a question on Cyber Campus\n",
        "    - Don't waste too much time on solving the error. The code is not thoroughly checked, and the error can be not your fault."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "095f3a9c",
      "metadata": {
        "id": "095f3a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2042c18-4f16-4db0-f211-32b2e1e281d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting koreanize-matplotlib\n",
            "  Downloading koreanize_matplotlib-0.1.1-py3-none-any.whl.metadata (992 bytes)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from koreanize-matplotlib) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->koreanize-matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->koreanize-matplotlib) (1.17.0)\n",
            "Downloading koreanize_matplotlib-0.1.1-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: koreanize-matplotlib\n",
            "Successfully installed koreanize-matplotlib-0.1.1\n"
          ]
        }
      ],
      "source": [
        "# If you are in Colab, install transformers\n",
        "!pip -q install transformers\n",
        "!pip install koreanize-matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "40ace9b0",
      "metadata": {
        "id": "40ace9b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1107333-d23a-436a-8740-edf31bd88aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-27 11:42:17--  https://raw.githubusercontent.com/jdasam/aat3020/2025/NLP_Assignment_4.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41934 (41K) [text/plain]\n",
            "Saving to: ‘NLP_Assignment_4.py’\n",
            "\n",
            "\rNLP_Assignment_4.py   0%[                    ]       0  --.-KB/s               \rNLP_Assignment_4.py 100%[===================>]  40.95K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-06-27 11:42:17 (53.8 MB/s) - ‘NLP_Assignment_4.py’ saved [41934/41934]\n",
            "\n",
            "--2025-06-27 11:42:17--  https://raw.githubusercontent.com/jdasam/aat3020/2025/assignment4_pre_defined.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13767 (13K) [text/plain]\n",
            "Saving to: ‘assignment4_pre_defined.py’\n",
            "\n",
            "assignment4_pre_def 100%[===================>]  13.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-27 11:42:17 (142 MB/s) - ‘assignment4_pre_defined.py’ saved [13767/13767]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the py file for submission\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020/2025/NLP_Assignment_4.py\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020/2025/assignment4_pre_defined.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8635fb79",
      "metadata": {
        "id": "8635fb79"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import koreanize_matplotlib\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn.utils.rnn import PackedSequence, pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "import os\n",
        "\n",
        "# Below helps to run tokenizer with multiprocessing\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd6024f",
      "metadata": {
        "id": "ecd6024f"
      },
      "source": [
        "#### Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f182b757",
      "metadata": {
        "id": "f182b757",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f93acf-c2b0-4369-ac4c-1932715c39a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.6967e+00,  4.4821e-01, -3.3510e+00,  ...,  5.7956e+00,\n",
              "          2.1540e+00,  1.4854e+00],\n",
              "        [-9.7154e-01, -1.3340e+01,  6.9144e+00,  ...,  1.2419e+00,\n",
              "         -2.6216e+00, -1.9333e+00],\n",
              "        [ 8.6621e-01, -9.9964e-02, -1.6763e+00,  ...,  2.3058e-01,\n",
              "         -4.1958e+00,  5.3794e+00],\n",
              "        ...,\n",
              "        [-4.9264e+00, -8.6890e-01,  5.3011e+00,  ..., -7.1469e+00,\n",
              "          8.2680e+00,  6.9801e+00],\n",
              "        [ 5.1104e+00,  1.4988e+01, -7.1690e+00,  ..., -6.0676e-01,\n",
              "         -3.5771e+00, -4.6799e+00],\n",
              "        [ 3.8025e+00,  2.4862e+00, -8.1009e-01,  ..., -9.4187e-03,\n",
              "         -1.7990e+00, -1.1718e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "This is the example of vectorization of dot product for two different sequence length.\n",
        "'''\n",
        "\n",
        "e_states = torch.randn(100, 16)\n",
        "d_states = torch.randn(80, 16)\n",
        "\n",
        "dot_product = torch.mm(e_states, d_states.permute(1,0)) # (100, 16) x (16, 80) = (100, 80)\n",
        "dot_product"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfefeae",
      "metadata": {
        "id": "fcfefeae"
      },
      "source": [
        "## Problem 1: Implement Dot Product Attention\n",
        "\n",
        "- Optimizing computation time is really important\n",
        "    - Use `torch.mm()` or `torch.matmul()`\n",
        "    - `torch.mm(a, b)` is a function for calculating matrix multiplcation of two matrices `a` and `b`\n",
        "        - `a` and `b` has to be 2-dim tensors\n",
        "        - `a.shape[1]` has to be equal to `b.shape[0]`\n",
        "    - `torch.matmul()` is a function for matrix multiplication but with broadcasting\n",
        "        - https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
        "        - It has less restriction on its input shape.\n",
        "            - It automatically matches the dimension of two tensors following some rules\n",
        "            - Therefore, it is a bit risky to use this funciton if you don't understand how it works\n",
        "- **DO NOT** use element-wise product or for loop!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2059e903",
      "metadata": {
        "id": "2059e903"
      },
      "source": [
        "### Hint: Dot product as matrix multiplcation.\n",
        "\n",
        "- Let's say there are two vector, $u=\\begin{bmatrix}-3 \\\\ 2 \\\\ 1\\end{bmatrix}$ and $v = \\begin{bmatrix} 5 \\\\ 4 \\\\ 6\\end{bmatrix}$\n",
        "    - The dot product of the two vectors is $(-3 \\times 5) + (2 \\times 4) + (1 \\times 6) = 1$\n",
        "    - It is equivalent to $u^T \\times v$\n",
        "        - In this case $u\\in\\mathbb{R}^{3\\times1}$ and $v\\in\\mathbb{R}^{3\\times1}$\n",
        "- In PyTorch, this can be described as below:\n",
        "    - `u = torch.Tensor([-3, 2, 1])`\n",
        "    - `v = torch.Tensor([5, 4, 6])`\n",
        "    - Dot product of u and v can be calculated by one of belows:\n",
        "        - `torch.mm(u.unsqueeze(0), v.unsqueeze(1))`\n",
        "            - `u.unsqueeze(0).shape == [1, 3]`\n",
        "            - `v.unsqueeze(1).shape == [3, 1]`\n",
        "            - `unsqueeze()` returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "            - The result has shape of [1,1]\n",
        "        - `torch.matmul(u, v)`\n",
        "        - `u @ v`\n",
        "            - `@` denotes matrix multiplication, which was introduced from Python 3.5\n",
        "        - `(u * v).sum()`\n",
        "            - This will be much slower than others, because it first do element-wise multiplcation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a4aed8a2",
      "metadata": {
        "id": "a4aed8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2454ef66-b0ae-43d2-807c-c9215b382ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of (u * v).sum() is -1.0. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\n",
            "Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is tensor([[-1.]])\n",
            "Result of torch.matmul(u, v) is -1.0\n",
            "Result of u @ v is -1.0\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Hint: Dot product as matrix multiplcation.\n",
        "'''\n",
        "\n",
        "u = torch.Tensor([-3, 2, 1])\n",
        "v = torch.Tensor([5, 4, 6])\n",
        "\n",
        "print(f\"Result of (u * v).sum() is {(u * v).sum()}. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\")\n",
        "print(f\"Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is {torch.mm(u.unsqueeze(0), v.unsqueeze(1))}\")\n",
        "print(f\"Result of torch.matmul(u, v) is {torch.matmul(u, v)}\")\n",
        "print(f\"Result of u @ v is {u @ v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15156e7",
      "metadata": {
        "id": "b15156e7"
      },
      "source": [
        "### 1-1 Get attention score with dot product (4 pts)\n",
        "- From a sequence of key and a single query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "66a20b45",
      "metadata": {
        "id": "66a20b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3d4325-53b7-4aea-e95e-a9e01680eadf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
              "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
              "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def get_attention_score_for_a_single_query(keys, query):\n",
        "  '''\n",
        "  This function returns an attention score for each vector in keys for a given query.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [T, C]. These are vectors that a query wants attend to\n",
        "    query (torch.Tensor): Has a shape of [C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [T]\n",
        "\n",
        "    attention_score[i] has to be a dot product value between keys[i] and query\n",
        "\n",
        "\n",
        "  TODO: Complete this sentence using torch.mm (matrix multiplication)\n",
        "  Hint: You can use atensor.unsqueeze(dim) to expand a dimension (with a diemsion of length 1) without changing item value of the tensor.\n",
        "  '''\n",
        "\n",
        "  expanded_query = query.unsqueeze(1) # [C] -> [C, 1]\n",
        "  attention_score_matrix = torch.mm(keys, expanded_query) # [T, C] x [C, 1] = [T, 1]\n",
        "  attention_score = attention_score_matrix.squeeze(1) # [T, 1] -> [T]\n",
        "\n",
        "  return attention_score\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_t, h_size)\n",
        "query = torch.randn(h_size)\n",
        "\n",
        "att_score = get_attention_score_for_a_single_query(keys, query)\n",
        "att_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ce66bf6c",
      "metadata": {
        "id": "ce66bf6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2c87ba-f17b-4c6c-a257-16dd0c6c3120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test Case\n",
        "'''\n",
        "assert att_score.ndim == 1 and len(att_score) == num_t, \"Error: Check output shape\"\n",
        "answer = torch.Tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
        "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
        "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])\n",
        "assert torch.allclose(att_score, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e9cd39",
      "metadata": {
        "id": "97e9cd39"
      },
      "source": [
        "### 1-2 Get attention weight from score (2pts)\n",
        "- Convert attention score to attention weight\n",
        "- Use softmax function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b63d332d",
      "metadata": {
        "id": "b63d332d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6052ea56-3e5a-4572-a5b5-733dd0a94eb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.7782e-06, 1.2936e-03, 8.8653e-04, 9.4370e-07, 4.0957e-03, 1.9541e-04,\n",
              "        5.5277e-05, 2.2321e-05, 1.9005e-04, 2.7829e-03, 1.2303e-04, 5.1099e-05,\n",
              "        1.7052e-04, 5.7296e-05, 3.5821e-04, 3.2593e-06, 1.1314e-05, 4.5893e-05,\n",
              "        1.5795e-05, 2.2107e-03, 8.2463e-05, 9.7556e-01, 1.1777e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def get_attention_weight_from_score(attention_score):\n",
        "  '''\n",
        "  This function converts attention score to attention weight.\n",
        "\n",
        "  Argument:\n",
        "    attention_score (torch.Tensor): Tensor of real number. Has a shape of [T]\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): Tensor of real number between 0 and 1. Sum of attention_weight is 1. Has a shape of [T]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "\n",
        "  assert attention_score.ndim == 1\n",
        "  attention_weight = torch.nn.functional.softmax(attention_score, dim=0)\n",
        "\n",
        "  return attention_weight\n",
        "\n",
        "att_weight = get_attention_weight_from_score(att_score)\n",
        "att_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8030b21a",
      "metadata": {
        "id": "8030b21a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2ab185-fe55-4681-fc47-2ef649f49803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "answer = torch.Tensor([0.0000,     0.0013,     0.0009,     0.0000,     0.0041,     0.0002,\n",
        "            0.0001,     0.0000,     0.0002,     0.0028,     0.0001,     0.0001,\n",
        "            0.0002,     0.0001,     0.0004,     0.0000,     0.0000,     0.0000,\n",
        "            0.0000,     0.0022,     0.0001,     0.9756,     0.0118])\n",
        "assert att_weight.shape == att_score.shape, 'Shape has to be remained the same'\n",
        "assert att_weight.sum() == 1, \"Sum of attention weight has to be 1\"\n",
        "assert torch.allclose(att_weight, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "\n",
        "print(\"Passed all the cases!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e2309e",
      "metadata": {
        "id": "26e2309e"
      },
      "source": [
        "### 1-3 Get weighted sum (4 pts)\n",
        "- Using attention weight and values, get the weighted sum\n",
        "- $a = \\sum_{i=1}^{T} \\alpha_i v_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6bf8e701",
      "metadata": {
        "id": "6bf8e701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8383971e-b755-4a55-c2d9-adb5b72f1c83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
              "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def get_weighted_sum(values, attention_weight):\n",
        "  '''\n",
        "  This function converts attention score to attention weight\n",
        "\n",
        "  Argument:\n",
        "    values (torch.Tensor): Has a shape of [T, C]. These are vectors that are used to form attention vector\n",
        "    attention_weight: Has a shape of [T], which represents the weight for each vector to compose the attention vector\n",
        "\n",
        "  Output:\n",
        "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight. Has a shape of [C]\n",
        "\n",
        "  TODO: Complete this function using torch.mm\n",
        "  '''\n",
        "\n",
        "  expanded_attention_weight = attention_weight.unsqueeze(0) # [T] -> [1, T]\n",
        "  attention_vector_matrix = torch.mm(expanded_attention_weight, values) # [1, T] x [T, C] = [1, C]\n",
        "  attention_vector = attention_vector_matrix.squeeze(0) # [1, C] -> [C]\n",
        "\n",
        "  return attention_vector\n",
        "\n",
        "att_vec = get_weighted_sum(keys, att_weight) # In simple dot-product-attention, key and value are the same\n",
        "att_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1253844f",
      "metadata": {
        "id": "1253844f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1247a88-39b4-4386-85e3-8e3266621397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases\n"
          ]
        }
      ],
      "source": [
        "answer = torch.Tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
        "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])\n",
        "assert att_vec.shape == query.shape, 'Shape has to be remained the same'\n",
        "assert torch.allclose(att_vec, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "print(\"Passed all the cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc00df77",
      "metadata": {
        "id": "cc00df77"
      },
      "source": [
        "## Problem 2: Attention in Batch ( 16 pts)\n",
        "- In this problem, you have to calculate attention with batch\n",
        "- You can use `torch.bmm()` for batch matrix multiplication https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "    - `torch.bmm()` takes two 3-dim tensor as its input\n",
        "    - Each tensor has to be 3-dim (atensor.ndim==3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "062446fe",
      "metadata": {
        "id": "062446fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8d7054-8fd7-4356-9f4e-783c6fccff55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix_left1: \n",
            "tensor([[ 1.5410, -0.2934, -2.1788],\n",
            "        [ 0.5684, -1.0845, -1.3986],\n",
            "        [ 0.4033,  0.8380, -0.7193],\n",
            "        [-0.4033, -0.5966,  0.1820],\n",
            "        [-0.8567,  1.1006, -1.0712]])\n",
            "matrix_left2: \n",
            "tensor([[ 0.1227, -0.5663,  0.3731],\n",
            "        [-0.8920, -1.5091,  0.3704],\n",
            "        [ 1.4565,  0.9398,  0.7748],\n",
            "        [ 0.1919,  1.2638, -1.2904],\n",
            "        [-0.7911, -0.0209, -0.7185]])\n",
            "matrix_right1: \n",
            "tensor([[ 0.5186, -1.3125,  0.1920,  0.5428],\n",
            "        [-2.2188,  0.2590, -1.0297, -0.5008],\n",
            "        [ 0.2734, -0.9181, -0.0404,  0.2881]])\n",
            "matrix_right2: \n",
            "tensor([[-0.0075, -0.9145, -1.0886, -0.2666],\n",
            "        [ 0.1894, -0.2190,  2.0576, -0.0354],\n",
            "        [ 0.0627, -0.7663,  1.0993,  2.7565]])\n",
            "Let's assume that we have batch of matrix, which is stack of these two matices\n",
            "matrix_left: \n",
            "tensor([[[ 1.5410, -0.2934, -2.1788],\n",
            "         [ 0.5684, -1.0845, -1.3986],\n",
            "         [ 0.4033,  0.8380, -0.7193],\n",
            "         [-0.4033, -0.5966,  0.1820],\n",
            "         [-0.8567,  1.1006, -1.0712]],\n",
            "\n",
            "        [[ 0.1227, -0.5663,  0.3731],\n",
            "         [-0.8920, -1.5091,  0.3704],\n",
            "         [ 1.4565,  0.9398,  0.7748],\n",
            "         [ 0.1919,  1.2638, -1.2904],\n",
            "         [-0.7911, -0.0209, -0.7185]]]) \n",
            " which is shape of torch.Size([2, 5, 3])\n",
            "matrix_right: \n",
            "tensor([[[ 0.5186, -1.3125,  0.1920,  0.5428],\n",
            "         [-2.2188,  0.2590, -1.0297, -0.5008],\n",
            "         [ 0.2734, -0.9181, -0.0404,  0.2881]],\n",
            "\n",
            "        [[-0.0075, -0.9145, -1.0886, -0.2666],\n",
            "         [ 0.1894, -0.2190,  2.0576, -0.0354],\n",
            "         [ 0.0627, -0.7663,  1.0993,  2.7565]]])\n",
            " which is shape of torch.Size([2, 3, 4])\n",
            "mat_mul_stack: \n",
            "tensor([[[ 0.8547, -0.0982,  0.6861,  0.3556],\n",
            "         [ 2.3188,  0.2571,  1.2824,  0.4487],\n",
            "         [-1.8468,  0.3480, -0.7564, -0.4080],\n",
            "         [ 1.1644,  0.2078,  0.5296,  0.1323],\n",
            "         [-3.1791,  2.3929, -1.2545, -1.3247]],\n",
            "\n",
            "        [[-0.0848, -0.2741, -0.8887,  1.0159],\n",
            "         [-0.2559,  0.8624, -1.7270,  1.3123],\n",
            "         [ 0.2156, -2.1316,  1.2000,  1.7143],\n",
            "         [ 0.1570,  0.5366,  0.9730, -3.6531],\n",
            "         [-0.0431,  1.2786,  0.0284, -1.7689]]])\n",
            " which is shape of torch.Size([2, 5, 4])\n",
            "mat_mul_bmm: \n",
            "tensor([[[ 0.8547, -0.0982,  0.6861,  0.3556],\n",
            "         [ 2.3188,  0.2571,  1.2824,  0.4487],\n",
            "         [-1.8468,  0.3480, -0.7564, -0.4080],\n",
            "         [ 1.1644,  0.2078,  0.5296,  0.1323],\n",
            "         [-3.1791,  2.3929, -1.2545, -1.3247]],\n",
            "\n",
            "        [[-0.0848, -0.2741, -0.8887,  1.0159],\n",
            "         [-0.2559,  0.8624, -1.7270,  1.3123],\n",
            "         [ 0.2156, -2.1316,  1.2000,  1.7143],\n",
            "         [ 0.1570,  0.5366,  0.9730, -3.6531],\n",
            "         [-0.0431,  1.2786,  0.0284, -1.7689]]])\n",
            " which is shape of torch.Size([2, 5, 4])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Hint for Problem 2\n",
        "\n",
        "You can calculate matrix multiplication of matrices in batch effectively using torch.bmm() or torch.matmul()\n",
        "'''\n",
        "\n",
        "torch.manual_seed(0)\n",
        "matrix_left1 = torch.randn(5, 3)\n",
        "matrix_left2 = torch.randn(5, 3)\n",
        "\n",
        "print(f\"matrix_left1: \\n{matrix_left1}\")\n",
        "print(f\"matrix_left2: \\n{matrix_left2}\")\n",
        "\n",
        "matrix_right1 = torch.randn(3, 4)\n",
        "matrix_right2 = torch.randn(3, 4)\n",
        "print(f\"matrix_right1: \\n{matrix_right1}\")\n",
        "print(f\"matrix_right2: \\n{matrix_right2}\")\n",
        "\n",
        "print(\"Let's assume that we have batch of matrix, which is stack of these two matices\")\n",
        "matrix_left = torch.stack([matrix_left1, matrix_left2])\n",
        "matrix_right = torch.stack([matrix_right1, matrix_right2])\n",
        "\n",
        "print(f\"matrix_left: \\n{matrix_left} \\n which is shape of {matrix_left.shape}\")\n",
        "print(f\"matrix_right: \\n{matrix_right}\\n which is shape of {matrix_right.shape}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Exhaustive method: using torch.mm() only with for loop (This is SLOW when matrix gets much larger)\n",
        "'''\n",
        "\n",
        "mm_forloop_output = []\n",
        "for sample_index in range(matrix_left.shape[0]):\n",
        "  mat_left = matrix_left[sample_index]\n",
        "  mat_right = matrix_right[sample_index]\n",
        "\n",
        "  mm_result = torch.mm(mat_left, mat_right)\n",
        "  mm_forloop_output.append(mm_result)\n",
        "\n",
        "mm_forloop_stack = torch.stack(mm_forloop_output)\n",
        "print(f\"mat_mul_stack: \\n{mm_forloop_stack}\\n which is shape of {mm_forloop_stack.shape}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Good method: using torch.bmm()\n",
        "'''\n",
        "\n",
        "mat_mul_bmm = torch.bmm(matrix_left, matrix_right)\n",
        "print(f\"mat_mul_bmm: \\n{mat_mul_bmm}\\n which is shape of {mat_mul_bmm.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd08038",
      "metadata": {
        "id": "9bd08038"
      },
      "source": [
        "### 2-1 Get attention score in batch (4 pts)\n",
        "- Now keys and query exist in batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fe689641",
      "metadata": {
        "id": "fe689641"
      },
      "outputs": [],
      "source": [
        "def get_attention_score_for_a_batch_query(keys, query):\n",
        "  '''\n",
        "  This function returns a batch of attention score for each vector in (multi-batch) keys for a given (single-batch) query.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [N, T, C]. These are vectors that a query wants attend to\n",
        "    query (torch.Tensor): Has a shape of [N, C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, T]\n",
        "\n",
        "    attention_score[n, i] has to be a dot product value between keys[n, i] and query[n]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: Use torch.bmm or torch.matmul after make two input tensors as 3-dim tensors.\n",
        "\n",
        "  '''\n",
        "\n",
        "  expanded_query = query.unsqueeze(2) # [N, C] -> [N, C, 1]\n",
        "  attention_score_matrix = torch.bmm(keys, expanded_query) # [N, T, C] x [N, C, 1] = [N, T, 1]\n",
        "  attention_score = attention_score_matrix.squeeze(2) # [N, T, 1] -> [N, T]\n",
        "\n",
        "  return attention_score\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b,num_t, h_size)\n",
        "query = torch.randn(num_b, h_size)\n",
        "out = get_attention_score_for_a_batch_query(keys, query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "59da18f0",
      "metadata": {
        "id": "59da18f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d16342-a98c-41b8-d4b6-be81fe78e8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "\n",
        "answer = torch.tensor([ -2.7744,   1.3793,   5.0969,   0.7559,   2.5898,  -0.9475,  -1.1960,\n",
        "           5.4975,   0.4018,   5.9949,  -5.9428,  -0.4441,   0.6729,  -0.8326,\n",
        "           3.7091,   1.4913,   2.2062,  -0.2244,  -4.0612,   2.9037,  10.6111,\n",
        "           4.1383,  -4.6549])\n",
        "\n",
        "assert out.ndim == 2 and out.shape == torch.Size([num_b, num_t]), \"Error: Check output shape\"\n",
        "assert torch.allclose(out[2], answer, atol=1e-4), \"Error: The output value is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a24922",
      "metadata": {
        "id": "25a24922"
      },
      "source": [
        "### 2-2 Get attention score in batch (4 pts)\n",
        "- Implement the same function but in batchified queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ae30995b",
      "metadata": {
        "id": "ae30995b",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94309b4-a67a-4f4b-e378-accf2924e736"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  6.1462,  -2.0761,  -3.7923,  ...,   2.9819,  -5.4682,  -1.0037],\n",
              "         [  2.7917,   1.5621,   0.6755,  ...,  12.6860,  -2.9139,   1.1092],\n",
              "         [  3.2981,  -4.6314,  -2.5517,  ...,   6.7435,  -0.3054,   1.3248],\n",
              "         ...,\n",
              "         [ -1.7558,  -2.9616,  -6.6489,  ...,   3.5522,   6.4960,   2.8827],\n",
              "         [ -1.9945,   5.0151,   2.2012,  ...,  -7.0258,  -1.4581,   3.8804],\n",
              "         [  1.7817,  -0.5098,   0.5882,  ...,   2.3800, -12.5525,  -5.4968]],\n",
              "\n",
              "        [[ -2.9907, -15.4518,   4.2723,  ...,   1.3169,  -7.9481,   1.8112],\n",
              "         [ -0.1470,   2.2430,  -0.6171,  ...,  -1.1141,   5.0494,  -3.9315],\n",
              "         [ -0.1703,   4.9486,   2.6252,  ...,  -1.6058,   0.7725,   1.6521],\n",
              "         ...,\n",
              "         [  2.4992,   4.4271,  -2.2281,  ...,   1.9466,   0.5016,  -0.2215],\n",
              "         [ -1.8304,  -4.3865,  -2.5648,  ...,   2.6665,  -3.3123,  -0.1541],\n",
              "         [  1.1768,  -9.2907,  -4.1481,  ...,  -4.1625,  -7.8802,  -6.4050]],\n",
              "\n",
              "        [[ -3.0427,  -5.7212,   1.0249,  ...,  10.2565,   9.3230,   6.1441],\n",
              "         [  1.6135,  -8.7972,   4.4344,  ...,   6.2974,  -1.0268,  -4.8747],\n",
              "         [ -0.4640,  -5.4037,   1.3304,  ...,  -2.4342,  11.0852,   0.2777],\n",
              "         ...,\n",
              "         [  5.9792,   3.7540,  -1.3935,  ..., -10.0146,  -3.6774,   7.1300],\n",
              "         [  2.2480,   0.4384,   2.5567,  ...,   3.6096,   4.8100,  -3.4380],\n",
              "         [ -3.4328,  -3.4441,  -3.9180,  ...,   1.5109,  -2.3779,   1.1166]],\n",
              "\n",
              "        [[ -5.6300,   2.6453,   3.2414,  ...,  -1.5721,  -4.5574,  -1.4957],\n",
              "         [  1.6126,  -0.9799,   5.6337,  ...,   2.2379,   3.2444,   0.2640],\n",
              "         [  2.5171,   0.6216,   3.7929,  ...,   4.6053,   2.8786,  -2.1453],\n",
              "         ...,\n",
              "         [  3.5575,  -3.8061,   0.3125,  ...,  -1.0397,   1.6313,  -2.5968],\n",
              "         [  3.1118,   3.2136,   1.4103,  ...,  -3.7134,   0.1190,   0.1317],\n",
              "         [  2.0304,  -0.5925,  -5.6959,  ...,  -4.8251,  -1.0030,   2.6204]],\n",
              "\n",
              "        [[ -4.5165,   0.5686,   6.2639,  ...,  -3.1651,   8.1099,   1.5312],\n",
              "         [ 11.9102,  -5.3511,   0.1570,  ...,  -8.5573,   3.2290,  -0.7118],\n",
              "         [ -1.3893,   2.3664,  -0.1588,  ...,  -2.6017,  -7.5239,   5.2198],\n",
              "         ...,\n",
              "         [ -4.5206,  -4.7446,   2.1696,  ...,  -2.2827,  -1.7162,   3.8161],\n",
              "         [  1.6530,   5.1878,  -5.3117,  ...,   2.3201,  -8.0600,  -3.3262],\n",
              "         [  9.5429,   1.1045,   3.4494,  ..., -11.6823,  -0.5222,  -2.4121]],\n",
              "\n",
              "        [[  4.3940,  -4.2766,  -0.5561,  ...,   0.8519,  -1.2661,  -2.1632],\n",
              "         [ -1.3437,   2.7691,  -5.9345,  ...,   1.8976,   4.2320,   1.4186],\n",
              "         [ -0.7749,  -5.5275,  -0.2756,  ...,   0.7429,  -1.3352,  -0.8282],\n",
              "         ...,\n",
              "         [  2.1454,  10.0679,  -2.7162,  ...,   0.3895,   0.2109,   2.8492],\n",
              "         [ -2.6441,  -3.9916,   2.0791,  ...,   5.3467,  -4.6615,  -1.6369],\n",
              "         [  0.9273,  10.4126,  -0.3644,  ...,   2.4843,   2.1014,  -0.5276]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def get_attention_score_for_a_batch_multiple_query(keys, queries):\n",
        "  '''\n",
        "  Now you have to implement the attention score for not only single query, but multiple queries.\n",
        "\n",
        "  This function returns a batch of attention score for each vector in keys for given queries.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while querys are hidden states over timestep of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that a query wants attend to\n",
        "    queries (torch.Tensor): Has a shape of [N, Tt, C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Tt, Ts]\n",
        "\n",
        "    attention_score[n, t, i] has to be a dot product value between keys[n, i] and query[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  HINT: Use torch.bmm() with proper transpose (permutation) of given tensors. (You can use atensor.permute())\n",
        "        Think about which dimension (axis) of tensors has to be multiplied together and resolved (disappear) after matrix multiplication,\n",
        "        and how the result tensor has to look like (shape)\n",
        "  '''\n",
        "  permuted_keys = keys.permute(0, 2, 1) # [N, Ts, C] -> [N, C, Ts]\n",
        "  attention_score = torch.bmm(queries, permuted_keys) # [N, Tt, C] x [N, C, Ts] = [N, Tt, Ts]\n",
        "\n",
        "  return attention_score\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_ts = 23\n",
        "num_tt = 14\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b, num_ts, h_size)\n",
        "queries = torch.randn(num_b, num_tt, h_size)\n",
        "att_score = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
        "\n",
        "att_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1cb45f70",
      "metadata": {
        "id": "1cb45f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ea08dc-dbd7-409e-83e9-a3db0b445ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([ 4.9620, -9.6091, -4.9472,  1.4543, -5.6273,  9.1436,  1.4172,  0.0464,\n",
        "        -5.7033,  4.5473,  7.7498,  1.3405, -3.1877,  2.8759])\n",
        "answer2 = torch.Tensor([[ 2.5171,  0.6216,  3.7929,  2.6163,  5.3290,  0.3592,  2.3067, -0.1099,\n",
        "         1.8963,  0.4175, -1.4283,  1.4388, -2.7825, -1.3690, -1.9615, -1.9514,\n",
        "        -6.4635,  1.9574,  0.1868,  8.5354,  4.6053,  2.8786, -2.1453]])\n",
        "assert att_score.ndim == 3 and att_score.shape == torch.Size([num_b, num_tt, num_ts]), 'Check the output shape'\n",
        "assert torch.allclose(att_score[2,:,4], answer, atol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_score[3,2,:], answer2, atol=1e-4),  'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ab274c",
      "metadata": {
        "id": "51ab274c"
      },
      "source": [
        "### 2-3 Get Masked Softmax (4 pts)\n",
        "- Implement masked softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "19ef2611",
      "metadata": {
        "id": "19ef2611",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66396cd5-98b4-4ed1-e63e-f5eff26002be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[8.5095e-02, 2.2856e-05, 4.1080e-06,  ..., 3.5946e-03,\n",
              "           7.6878e-07, 6.6794e-05],\n",
              "          [5.0265e-05, 1.4698e-05, 6.0563e-06,  ..., 9.9608e-01,\n",
              "           1.6724e-07, 9.3445e-06],\n",
              "          [2.6409e-02, 9.5063e-06, 7.6066e-05,  ..., 8.2803e-01,\n",
              "           7.1906e-04, 3.6707e-03],\n",
              "          ...,\n",
              "          [1.8632e-04, 5.5794e-05, 1.3970e-06,  ..., 3.7625e-02,\n",
              "           7.1448e-01, 1.9264e-02],\n",
              "          [4.1884e-05, 4.6370e-02, 2.7809e-03,  ..., 2.7350e-07,\n",
              "           7.1609e-05, 1.4910e-02],\n",
              "          [1.4554e-03, 1.4716e-04, 4.4123e-04,  ..., 2.6474e-03,\n",
              "           8.6639e-10, 1.0046e-06]],\n",
              " \n",
              "         [[1.4556e-06, 5.6399e-12, 2.0765e-03,  ..., 1.0810e-04,\n",
              "           1.0234e-08, 1.7722e-04],\n",
              "          [7.2275e-05, 7.8883e-04, 4.5171e-05,  ..., 2.7478e-05,\n",
              "           1.3054e-02, 1.6422e-06],\n",
              "          [8.3620e-06, 1.3978e-03, 1.3690e-04,  ..., 1.9901e-06,\n",
              "           2.1468e-05, 5.1737e-05],\n",
              "          ...,\n",
              "          [7.2220e-02, 4.9651e-01, 6.3913e-04,  ..., 4.1556e-02,\n",
              "           9.7970e-03, 4.7540e-03],\n",
              "          [2.6008e-04, 2.0184e-05, 1.2479e-04,  ..., 2.3341e-02,\n",
              "           5.9093e-05, 1.3903e-03],\n",
              "          [2.2393e-03, 6.3696e-08, 1.0902e-05,  ..., 1.0746e-05,\n",
              "           2.6103e-07, 1.1412e-06]],\n",
              " \n",
              "         [[1.0078e-06, 6.9203e-08, 5.8871e-05,  ..., 6.0139e-01,\n",
              "           2.3644e-01, 9.8438e-03],\n",
              "          [4.8125e-03, 1.4490e-07, 8.0813e-02,  ..., 5.2067e-01,\n",
              "           3.4332e-04, 7.3208e-06],\n",
              "          [9.6156e-06, 6.8819e-08, 5.7844e-05,  ..., 1.3407e-06,\n",
              "           9.9705e-01, 2.0188e-05],\n",
              "          ...,\n",
              "          [1.0299e-02, 1.1127e-03, 6.4695e-06,  ..., 1.1662e-09,\n",
              "           6.5917e-07, 3.2553e-02],\n",
              "          [1.1371e-03, 1.8616e-04, 1.5483e-03,  ..., 4.4373e-03,\n",
              "           1.4739e-02, 3.8583e-06],\n",
              "          [1.3092e-04, 1.2944e-04, 8.0587e-05,  ..., 1.8367e-02,\n",
              "           3.7597e-04, 1.2382e-02]],\n",
              " \n",
              "         [[3.3037e-07, 1.2970e-03, 2.3540e-03,  ..., 1.9115e-05,\n",
              "           9.6572e-07, 2.0632e-05],\n",
              "          [1.4234e-03, 1.0651e-04, 7.9368e-02,  ..., 2.6601e-03,\n",
              "           7.2776e-03, 3.6950e-04],\n",
              "          [2.2441e-03, 3.3717e-04, 8.0376e-03,  ..., 1.8111e-02,\n",
              "           3.2214e-03, 2.1194e-05],\n",
              "          ...,\n",
              "          [1.1613e-01, 7.3622e-05, 4.5255e-03,  ..., 1.1706e-03,\n",
              "           1.6921e-02, 2.4671e-04],\n",
              "          [1.2482e-01, 1.3819e-01, 2.2768e-02,  ..., 1.3556e-04,\n",
              "           6.2594e-03, 6.3394e-03],\n",
              "          [4.2618e-04, 3.0935e-05, 1.8797e-07,  ..., 4.4903e-07,\n",
              "           2.0521e-05, 7.6880e-04]],\n",
              " \n",
              "         [[5.4761e-06, 8.8486e-04, 2.6322e-01,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [9.9943e-01, 3.1861e-08, 7.8598e-06,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [7.3903e-05, 3.1606e-03, 2.5298e-04,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [3.6373e-06, 2.9076e-06, 2.9262e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [2.0599e-02, 7.0629e-01, 1.9459e-05,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [9.4030e-01, 2.0348e-04, 2.1226e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              " \n",
              "         [[2.5799e-01, 4.4262e-05, 1.8274e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [7.1575e-05, 4.3747e-03, 7.2610e-07,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [5.9149e-03, 5.1039e-05, 9.7449e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [3.4783e-04, 9.5951e-01, 2.6915e-06,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [3.0966e-04, 8.0478e-05, 3.4847e-02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [7.3722e-05, 9.7059e-01, 2.0260e-05,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]]]),\n",
              " tensor([[[8.5095e-02, 2.2856e-05, 4.1080e-06,  ..., 3.5946e-03,\n",
              "           7.6878e-07, 6.6794e-05],\n",
              "          [5.0265e-05, 1.4698e-05, 6.0563e-06,  ..., 9.9608e-01,\n",
              "           1.6724e-07, 9.3445e-06],\n",
              "          [2.6409e-02, 9.5063e-06, 7.6066e-05,  ..., 8.2803e-01,\n",
              "           7.1906e-04, 3.6707e-03],\n",
              "          ...,\n",
              "          [1.8632e-04, 5.5794e-05, 1.3970e-06,  ..., 3.7625e-02,\n",
              "           7.1448e-01, 1.9264e-02],\n",
              "          [4.1884e-05, 4.6370e-02, 2.7809e-03,  ..., 2.7350e-07,\n",
              "           7.1609e-05, 1.4910e-02],\n",
              "          [1.4554e-03, 1.4716e-04, 4.4123e-04,  ..., 2.6474e-03,\n",
              "           8.6639e-10, 1.0046e-06]],\n",
              " \n",
              "         [[1.4556e-06, 5.6399e-12, 2.0765e-03,  ..., 1.0810e-04,\n",
              "           1.0234e-08, 1.7722e-04],\n",
              "          [7.2275e-05, 7.8883e-04, 4.5171e-05,  ..., 2.7478e-05,\n",
              "           1.3054e-02, 1.6422e-06],\n",
              "          [8.3620e-06, 1.3978e-03, 1.3690e-04,  ..., 1.9901e-06,\n",
              "           2.1468e-05, 5.1737e-05],\n",
              "          ...,\n",
              "          [7.2220e-02, 4.9651e-01, 6.3913e-04,  ..., 4.1556e-02,\n",
              "           9.7970e-03, 4.7540e-03],\n",
              "          [2.6008e-04, 2.0184e-05, 1.2479e-04,  ..., 2.3341e-02,\n",
              "           5.9093e-05, 1.3903e-03],\n",
              "          [2.2393e-03, 6.3696e-08, 1.0902e-05,  ..., 1.0746e-05,\n",
              "           2.6103e-07, 1.1412e-06]],\n",
              " \n",
              "         [[1.0078e-06, 6.9203e-08, 5.8871e-05,  ..., 6.0139e-01,\n",
              "           2.3644e-01, 9.8438e-03],\n",
              "          [4.8125e-03, 1.4490e-07, 8.0813e-02,  ..., 5.2067e-01,\n",
              "           3.4332e-04, 7.3208e-06],\n",
              "          [9.6156e-06, 6.8819e-08, 5.7844e-05,  ..., 1.3407e-06,\n",
              "           9.9705e-01, 2.0188e-05],\n",
              "          ...,\n",
              "          [1.0299e-02, 1.1127e-03, 6.4695e-06,  ..., 1.1662e-09,\n",
              "           6.5917e-07, 3.2553e-02],\n",
              "          [1.1371e-03, 1.8616e-04, 1.5483e-03,  ..., 4.4373e-03,\n",
              "           1.4739e-02, 3.8583e-06],\n",
              "          [1.3092e-04, 1.2944e-04, 8.0587e-05,  ..., 1.8367e-02,\n",
              "           3.7597e-04, 1.2382e-02]],\n",
              " \n",
              "         [[3.3037e-07, 1.2970e-03, 2.3540e-03,  ..., 1.9115e-05,\n",
              "           9.6572e-07, 2.0632e-05],\n",
              "          [1.4234e-03, 1.0651e-04, 7.9368e-02,  ..., 2.6601e-03,\n",
              "           7.2776e-03, 3.6950e-04],\n",
              "          [2.2441e-03, 3.3717e-04, 8.0376e-03,  ..., 1.8111e-02,\n",
              "           3.2214e-03, 2.1194e-05],\n",
              "          ...,\n",
              "          [1.1613e-01, 7.3622e-05, 4.5255e-03,  ..., 1.1706e-03,\n",
              "           1.6921e-02, 2.4671e-04],\n",
              "          [1.2482e-01, 1.3819e-01, 2.2768e-02,  ..., 1.3556e-04,\n",
              "           6.2594e-03, 6.3394e-03],\n",
              "          [4.2618e-04, 3.0935e-05, 1.8797e-07,  ..., 4.4903e-07,\n",
              "           2.0521e-05, 7.6880e-04]],\n",
              " \n",
              "         [[5.4761e-06, 8.8486e-04, 2.6322e-01,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [9.9943e-01, 3.1861e-08, 7.8598e-06,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [7.3903e-05, 3.1606e-03, 2.5298e-04,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [3.6373e-06, 2.9076e-06, 2.9262e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [2.0599e-02, 7.0629e-01, 1.9459e-05,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [9.4030e-01, 2.0348e-04, 2.1226e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              " \n",
              "         [[2.5799e-01, 4.4262e-05, 1.8274e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [7.1575e-05, 4.3747e-03, 7.2610e-07,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [5.9149e-03, 5.1039e-05, 9.7449e-03,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [3.4783e-04, 9.5951e-01, 2.6915e-06,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [3.0966e-04, 8.0478e-05, 3.4847e-02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [7.3722e-05, 9.7059e-01, 2.0260e-05,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "def get_masked_softmax(attention_score, mask):\n",
        "  '''\n",
        "  During the batch computation, each sequence in the batch can have different length.\n",
        "  To group them as in a single tensor, we usually pad values\n",
        "\n",
        "  Arguments:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Tt, Ts]\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, Ts] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, i] == 1 if and only if input_batch[n,i] is not a padded value.\n",
        "                         If input_batch[n,i] is a padded value, then mask[n,i] == 0\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
        "                                    Has a shape of [N, Tt, Ts]\n",
        "\n",
        "    attention_weight[n, t, i] has to be an attention weight of values[n, i] for queries[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: You can give -infinity value by -float(\"inf\")\n",
        "  Caution: Do not directly replace the \"attention_score\" tensor. Use atensor.clone()\n",
        "  '''\n",
        "\n",
        "  masked_attention_score = attention_score.clone()\n",
        "  expanded_mask = mask.unsqueeze(1) # [N, Ts] -> [N, 1, Ts]\n",
        "  masked_attention_score.masked_fill_(expanded_mask == 0, -float(\"inf\"))\n",
        "  attention_weight = torch.nn.functional.softmax(masked_attention_score, dim=-1)\n",
        "\n",
        "  return attention_weight\n",
        "\n",
        "\n",
        "'''\n",
        "Don't change this codes\n",
        "'''\n",
        "mask = torch.ones_like(att_score)[:, 0]\n",
        "mask[4, 15:] = 0\n",
        "mask[5, 17:] = 0\n",
        "att_score_modified = att_score.clone()\n",
        "att_score_modified[4, 15:] = 0\n",
        "attention_weight = get_masked_softmax(att_score, mask)\n",
        "attention_weight_for_modified = get_masked_softmax(att_score_modified, mask)\n",
        "attention_weight, attention_weight_for_modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "99abd97a",
      "metadata": {
        "id": "99abd97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b024be3f-e77a-4ce6-eee2-5e8df26f84cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "answer = torch.Tensor([0.0120,     0.0002,     0.0901,     0.0003,     0.0259,     0.0036,\n",
        "            0.5617,     0.0108,     0.2508,     0.0054,     0.0001,     0.0010,\n",
        "            0.0000,     0.0005,     0.0375,     0.0000,     0.0000,     0.0000,\n",
        "            0.0000,     0.0000,     0.0000,     0.0000,     0.0000])\n",
        "assert torch.allclose(attention_weight[4,3], answer, atol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(attention_weight.sum(2),  torch.tensor([1.0]) , atol=1e-6 ), 'Sum of attention weight has to be 1'\n",
        "assert torch.allclose(attention_weight, attention_weight_for_modified), \"Output is different even though only masked part is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2030f5",
      "metadata": {
        "id": "ca2030f5"
      },
      "source": [
        "### 2-4 Implement weighted sum in batchified version (4 pts)\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9b2e5c36",
      "metadata": {
        "id": "9b2e5c36",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e406e250-d3ba-4407-cf7c-dfd74a9a05bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.3373e-01, -2.0938e+00, -4.5334e-02,  ...,  9.5458e-01,\n",
              "           1.0229e+00,  1.1447e+00],\n",
              "         [-1.3264e-01, -5.9684e-02,  4.0668e-01,  ..., -2.2277e+00,\n",
              "           1.4654e+00, -1.2140e+00],\n",
              "         [-2.1483e-01, -3.9624e-02,  3.4638e-01,  ..., -1.8784e+00,\n",
              "           1.3381e+00, -1.0378e+00],\n",
              "         ...,\n",
              "         [ 4.7269e-01,  2.7082e+00, -1.5966e-01,  ..., -8.5101e-01,\n",
              "           1.6295e+00, -6.9686e-01],\n",
              "         [ 1.3049e+00, -7.9595e-01,  5.8837e-01,  ..., -4.2750e-01,\n",
              "           9.4185e-02, -6.0422e-01],\n",
              "         [-2.1447e-01,  4.5967e-01,  9.0154e-01,  ..., -1.1010e+00,\n",
              "           1.5383e+00, -6.2885e-01]],\n",
              "\n",
              "        [[-2.0093e-02,  9.5785e-01, -1.0546e+00,  ..., -5.6727e-01,\n",
              "           9.5331e-01, -1.4754e+00],\n",
              "         [ 2.6579e-01, -9.2264e-01,  8.4929e-01,  ...,  2.4586e+00,\n",
              "          -2.5894e+00,  2.3314e+00],\n",
              "         [ 5.3756e-01, -9.0355e-01,  1.7743e-01,  ...,  2.2658e+00,\n",
              "           3.0777e-01,  2.0951e+00],\n",
              "         ...,\n",
              "         [ 1.3244e-01,  8.5970e-01,  2.4145e-01,  ..., -1.7893e-01,\n",
              "           9.2332e-01, -6.8233e-01],\n",
              "         [-8.8633e-02,  5.1850e-01, -1.4503e+00,  ..., -5.1185e-01,\n",
              "           5.6031e-01,  8.2076e-01],\n",
              "         [-1.1267e-01, -5.4648e-01, -4.8170e-02,  ...,  1.8755e+00,\n",
              "          -3.0700e-01,  9.4322e-01]],\n",
              "\n",
              "        [[-6.1817e-01,  2.6989e-01, -1.7159e+00,  ..., -1.2042e+00,\n",
              "          -1.3739e+00,  6.5000e-01],\n",
              "         [ 8.7003e-01,  8.7786e-02, -1.3764e+00,  ..., -1.0438e+00,\n",
              "          -1.3338e-01,  4.9795e-01],\n",
              "         [-5.9019e-01,  4.2505e-01, -1.2334e+00,  ..., -7.7825e-01,\n",
              "          -2.1349e+00,  3.3232e-01],\n",
              "         ...,\n",
              "         [-4.5776e-01,  5.3027e-01,  8.0296e-01,  ...,  1.1601e+00,\n",
              "          -4.1216e-01, -8.4225e-01],\n",
              "         [-2.7609e-01,  5.8881e-01,  5.0533e-01,  ..., -5.1114e-01,\n",
              "           1.7841e+00, -5.5431e-01],\n",
              "         [ 2.9698e-02, -1.3222e+00, -7.9683e-01,  ...,  3.8490e-01,\n",
              "          -1.0175e+00,  1.4906e-01]],\n",
              "\n",
              "        [[ 8.3192e-01,  1.6496e-01, -9.2035e-01,  ..., -1.6308e+00,\n",
              "          -5.2375e-01, -1.1106e+00],\n",
              "         [-7.9787e-01, -3.0595e-01, -9.7095e-01,  ...,  2.6117e-03,\n",
              "           2.1266e-01,  7.9024e-01],\n",
              "         [ 3.6286e-01,  6.3939e-01,  3.0618e-01,  ...,  1.3938e+00,\n",
              "           2.0934e+00,  1.1223e+00],\n",
              "         ...,\n",
              "         [ 4.2950e-01,  2.7868e-01, -6.7143e-01,  ..., -4.3766e-01,\n",
              "          -7.7644e-01, -2.6162e-01],\n",
              "         [-8.4428e-01, -5.4181e-02, -1.7641e-01,  ...,  1.1040e-01,\n",
              "           1.6086e-01,  7.0424e-01],\n",
              "         [ 1.2658e+00,  1.9420e+00, -1.2539e-01,  ...,  1.0030e+00,\n",
              "          -1.3475e-01, -5.0187e-01]],\n",
              "\n",
              "        [[ 5.8191e-03, -5.3975e-01, -3.6032e-01,  ...,  5.1161e-01,\n",
              "           1.4710e-01,  1.0211e+00],\n",
              "         [ 3.7232e+00,  1.7317e+00, -7.4160e-01,  ..., -4.5058e-01,\n",
              "          -2.1989e+00,  1.4684e+00],\n",
              "         [-7.7349e-01,  1.1662e+00, -1.5678e+00,  ..., -5.2115e-01,\n",
              "           1.4653e+00, -1.6642e+00],\n",
              "         ...,\n",
              "         [-4.5703e-01,  1.6784e+00, -6.8821e-02,  ..., -3.6136e-01,\n",
              "          -6.9498e-01, -6.5073e-01],\n",
              "         [-1.2242e+00,  8.1537e-01,  1.2802e+00,  ...,  2.8726e-01,\n",
              "          -6.7655e-01,  9.0936e-01],\n",
              "         [ 3.4806e+00,  1.7345e+00, -6.9836e-01,  ..., -4.4237e-01,\n",
              "          -2.1103e+00,  1.3477e+00]],\n",
              "\n",
              "        [[ 7.4298e-01,  6.8085e-01, -4.0109e-01,  ..., -6.9539e-01,\n",
              "          -7.1683e-01,  1.3663e-01],\n",
              "         [-1.7923e-01, -4.7257e-01, -4.0078e-01,  ..., -8.6471e-01,\n",
              "           1.2872e-01, -2.4034e-02],\n",
              "         [-3.2743e-01, -1.8710e-01,  6.0103e-02,  ...,  6.2887e-01,\n",
              "          -8.4385e-02,  4.1515e-01],\n",
              "         ...,\n",
              "         [-2.2279e-01,  2.0214e-01, -1.1098e+00,  ..., -2.4794e+00,\n",
              "          -1.3261e+00,  1.8497e+00],\n",
              "         [-4.9566e-01, -1.3428e+00,  4.1454e-01,  ..., -7.3784e-01,\n",
              "           3.7670e-01, -1.1959e-01],\n",
              "         [-2.2121e-01,  1.5855e-01, -1.1392e+00,  ..., -2.5089e+00,\n",
              "          -1.3221e+00,  1.8876e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "def get_batch_weighted_sum(values, attention_weight):\n",
        "  '''\n",
        "  This function converts attention score to attention weight\n",
        "\n",
        "  Argument:\n",
        "    values (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that are used to form attention vector\n",
        "    attention_weight: Has a shape of [N, Ts, Tt], which represents the weight for each vector to compose the attention vector\n",
        "                      attention_weight[n, s, t] represents weight for value[n, s] that corresponds to a given query, queries[n, t]\n",
        "\n",
        "  Output:\n",
        "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight.\n",
        "                                     Has a shape of [N, Tt, C]\n",
        "\n",
        "  TODO: Complete this function using torch.bmm\n",
        "  '''\n",
        "  attention_vector = torch.bmm(attention_weight, values) # [N, Tt, Ts] x [N, Ts, C] = [N, Tt, C]\n",
        "\n",
        "  return attention_vector\n",
        "\n",
        "att_out = get_batch_weighted_sum(keys, attention_weight)\n",
        "att_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c93231ea",
      "metadata": {
        "id": "c93231ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88296cf7-74b5-42c1-b1cc-874be11dc662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([-0.9348, -1.2628, -0.9189, -0.3434, -1.6476,  0.1031, -0.6963, -0.7462,\n",
        "         0.1484,  0.6810,  0.7950,  1.0277, -1.5988,  0.4232, -1.5540,  0.1801])\n",
        "answer2 = torch.Tensor([-0.9204, -0.9710,  0.3062, -1.0122,  1.1933,  0.1302, -1.0280,  0.0095,\n",
        "         0.6124,  0.0615, -1.2312, -0.6714, -0.1764, -0.1254])\n",
        "assert att_out.ndim == 3 and att_out.shape == torch.Size([num_b, num_tt, h_size]), 'Check the output shape'\n",
        "assert torch.max(torch.abs(att_out[2, 5] - answer)) < 1e-4, 'Calculated result is wrong'\n",
        "assert torch.max(torch.abs(att_out[3,:,2] - answer2)) < 1e-4,  'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebbdbf2b",
      "metadata": {
        "id": "ebbdbf2b"
      },
      "source": [
        "## Problem 3: Make seq2seq with attention (14 pts)\n",
        "- Using Pre-defined `TranslatorBi` class, complete a new `TranslatorAtt` class\n",
        "- If you implement it correctly, you can translate a sentence and get the corresponding attention map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904d44a3",
      "metadata": {
        "id": "904d44a3"
      },
      "source": [
        "### 3-0 Prepare dataset and tokenizer\n",
        "- To use the pretrained model correctly, you can use the pretrained vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9c670709",
      "metadata": {
        "id": "9c670709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8393b17f-7729-4db1-b777-8532eef94fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "unzip:  cannot find or open nia_korean_english_csv.zip, nia_korean_english_csv.zip.zip or nia_korean_english_csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "!gdown 13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
        "!unzip -q nia_korean_english_csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/"
      ],
      "metadata": {
        "id": "2hQiAFvGMqZr",
        "outputId": "cd03dafe-937a-4a1a-a8d1-8f526830f611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2hQiAFvGMqZr",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 318540\n",
            "-rw-r--r-- 1 root root     13767 Jun 27 11:42 assignment4_pre_defined.py\n",
            "-rw-r--r-- 1 root root 105906176 Jun 27 11:43 assignment4_values.pt\n",
            "-rw-r--r-- 1 root root 111149056 Jun 27 11:43 kor_eng_translator_attention_model_best.pt\n",
            "-rw-r--r-- 1 root root 109051904 Jun 27 11:43 nia_korean_english_csv.zip\n",
            "-rw-r--r-- 1 root root     41934 Jun 27 11:42 NLP_Assignment_4.py\n",
            "drwxr-xr-x 1 root root      4096 Jun 25 13:36 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q nia_korean_english_csv.zip"
      ],
      "metadata": {
        "id": "75ssu5KGk8Yb"
      },
      "id": "75ssu5KGk8Yb",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "23f4d086",
      "metadata": {
        "id": "23f4d086"
      },
      "source": [
        "- You can train your own tokenizer with the following code\n",
        "- Otherwise, you can skip to the next cell to use the pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411e7832",
      "metadata": {
        "id": "411e7832",
        "outputId": "22770b79-4bb9-4058-edf9-056ef8ec5cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n!gdown 1GMFWREWBVcD5vJdwNFadHmzVStclcyKf\\n!unzip -q nia-aihub-korean-english-txt.zip\\n\\nfrom tokenizers import BertWordPieceTokenizer\\ntokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\\n\\ndata_list = list(Path('nia_korean_english').glob('*.xlsx'))\\n\\ncorpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list]\\nen_corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list]\\n\\nvocab_size    = 32000  # Number of maximum size of the vocabulary\\nlimit_alphabet= 6000\\noutput_dir    = Path('hugging_kor_%d'%(vocab_size))\\nen_output_dir = Path('hugging_eng_%d'%(vocab_size))\\noutput_dir.mkdir(exist_ok=True)\\nen_output_dir.mkdir(exist_ok=True)\\nmin_frequency = 5\\n\\ntokenizer.train(files=corpus_file,\\n               vocab_size=vocab_size,\\n               min_frequency=min_frequency,\\n               limit_alphabet=limit_alphabet,\\n               show_progress=True)\\n\\ntokenizer.save_model(str(output_dir))\\n\\nen_tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\\nen_tokenizer.train(files=en_corpus_file,\\n                vocab_size=vocab_size,\\n                min_frequency=min_frequency,\\n                limit_alphabet=limit_alphabet,\\n                show_progress=True)\\nen_tokenizer.save_model(str(en_output_dir))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "'''\n",
        "Caution: Training your own tokenizer will make mismatch between the pretrained model weights and your tokenizer.\n",
        "This will cause the below assertion not working.\n",
        "So use this only for testing your own tokenizer.\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "!gdown 1GMFWREWBVcD5vJdwNFadHmzVStclcyKf\n",
        "!unzip -q nia-aihub-korean-english-txt.zip\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "\n",
        "data_list = list(Path('nia_korean_english').glob('*.xlsx'))\n",
        "\n",
        "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list]\n",
        "en_corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list]\n",
        "\n",
        "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
        "limit_alphabet= 6000\n",
        "output_dir    = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_output_dir = Path('hugging_eng_%d'%(vocab_size))\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "en_output_dir.mkdir(exist_ok=True)\n",
        "min_frequency = 5\n",
        "\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,\n",
        "               limit_alphabet=limit_alphabet,\n",
        "               show_progress=True)\n",
        "\n",
        "tokenizer.save_model(str(output_dir))\n",
        "\n",
        "en_tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "en_tokenizer.train(files=en_corpus_file,\n",
        "                vocab_size=vocab_size,\n",
        "                min_frequency=min_frequency,\n",
        "                limit_alphabet=limit_alphabet,\n",
        "                show_progress=True)\n",
        "en_tokenizer.save_model(str(en_output_dir))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d4edd48e",
      "metadata": {
        "id": "d4edd48e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711771eb-8bb0-4dd6-92f8-30ba4adebfbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요', '.', '서강', '##대학교', '##에', '오신', '걸', '환영', '##합니다', '.']\n",
            "['hello', '.', 'welcome', 'to', 'so', '##gang', 'university', '.']\n"
          ]
        }
      ],
      "source": [
        "# Load data and tokenizer\n",
        "\n",
        "df = pd.read_csv('nia_korean_english.csv')\n",
        "\n",
        "src_tokenizer = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False)\n",
        "tgt_tokenizer = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False)\n",
        "\n",
        "\n",
        "# Test tokenizer\n",
        "print(src_tokenizer.tokenize(\"안녕하세요. 서강대학교에 오신 걸 환영합니다.\"))\n",
        "print(tgt_tokenizer.tokenize(\"Hello. Welcome to Sogang University.\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ab8fa529",
      "metadata": {
        "id": "ab8fa529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5acc246-8fd8-4aab-e61c-80a4a3a721b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Item Example: (tensor([    2,    11,    70,  4665,  5209, 13306,    71, 12901,  9565, 12435,\n",
            "           11,  3546, 14567,  4325,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  4996,  3397,  6461,    18,     3]), tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18]), tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3]))\n",
            "Length of split : Train 1442176, Valid 80120, Test 80122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PackedSequence(data=tensor([   2,    2,    2,  ..., 7076,   18,    3]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 63, 60, 57, 55, 52, 50, 48, 42, 38, 36,\n",
              "         35, 34, 33, 33, 31, 31, 31, 31, 29, 26, 23, 21, 18, 18, 17, 15, 14, 13,\n",
              "         11, 10,  8,  8,  7,  6,  6,  6,  6,  5,  4,  3,  3,  3,  2,  2,  1,  1,\n",
              "          1,  1]), sorted_indices=tensor([55, 42, 60, 26, 37, 29, 20,  7,  1, 33,  6,  9, 56, 23, 47, 57,  0, 49,\n",
              "         15, 41, 24, 38, 14, 18, 46,  4, 13, 44, 61, 50, 40,  2, 52, 21, 39, 51,\n",
              "         10, 53, 63, 31, 19, 22,  3, 36,  8, 11, 12, 34, 62,  5, 43, 59, 45, 17,\n",
              "         54, 30, 32, 28, 25, 16, 35, 58, 27, 48]), unsorted_indices=tensor([16,  8, 31, 42, 25, 49, 10,  7, 44, 11, 36, 45, 46, 26, 22, 18, 59, 53,\n",
              "         23, 40,  6, 33, 41, 13, 20, 58,  3, 62, 57,  5, 55, 39, 56,  9, 47, 60,\n",
              "         43,  4, 21, 34, 30, 19,  1, 50, 27, 52, 24, 14, 63, 17, 29, 35, 32, 37,\n",
              "         54,  0, 12, 15, 61, 51,  2, 28, 48, 38])),\n",
              " PackedSequence(data=tensor([   2,    2,    2,  ...,   18, 1913,   18]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 62, 61, 58, 55, 54, 52, 49, 48, 45, 43,\n",
              "         41, 38, 35, 33, 33, 33, 33, 32, 32, 31, 30, 30, 30, 29, 28, 27, 24, 23,\n",
              "         19, 18, 17, 16, 14, 13, 11, 11, 10, 10,  9,  7,  7,  5,  5,  5,  5,  4,\n",
              "          4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  1,  1]), sorted_indices=tensor([42, 55, 26,  7,  1, 37, 60, 49, 18,  0, 33, 23, 29, 24, 57,  6, 46,  9,\n",
              "         41, 40, 38, 47, 61, 56, 50, 20, 15,  4, 14, 44, 13, 52, 21,  2, 11, 53,\n",
              "         63, 10, 39,  5, 31, 34,  8, 62, 54, 45, 51, 22,  3, 28, 43, 36, 19, 59,\n",
              "         25, 16, 12, 35, 48, 30, 58, 17, 27, 32]), unsorted_indices=tensor([ 9,  4, 33, 48, 27, 39, 15,  3, 42, 17, 37, 34, 56, 30, 28, 26, 55, 61,\n",
              "          8, 52, 25, 32, 47, 11, 13, 54,  2, 62, 49, 12, 59, 40, 63, 10, 41, 57,\n",
              "         51,  5, 20, 38, 19, 18,  0, 50, 29, 45, 16, 21, 58,  7, 24, 46, 31, 35,\n",
              "         44,  1, 23, 14, 60, 53,  6, 22, 43, 36])),\n",
              " PackedSequence(data=tensor([16406,  1023, 26808,  ...,     3,    18,     3]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 62, 61, 58, 55, 54, 52, 49, 48, 45, 43,\n",
              "         41, 38, 35, 33, 33, 33, 33, 32, 32, 31, 30, 30, 30, 29, 28, 27, 24, 23,\n",
              "         19, 18, 17, 16, 14, 13, 11, 11, 10, 10,  9,  7,  7,  5,  5,  5,  5,  4,\n",
              "          4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  1,  1]), sorted_indices=tensor([42, 55, 26,  7,  1, 37, 60, 49, 18,  0, 33, 23, 29, 24, 57,  6, 46,  9,\n",
              "         41, 40, 38, 47, 61, 56, 50, 20, 15,  4, 14, 44, 13, 52, 21,  2, 11, 53,\n",
              "         63, 10, 39,  5, 31, 34,  8, 62, 54, 45, 51, 22,  3, 28, 43, 36, 19, 59,\n",
              "         25, 16, 12, 35, 48, 30, 58, 17, 27, 32]), unsorted_indices=tensor([ 9,  4, 33, 48, 27, 39, 15,  3, 42, 17, 37, 34, 56, 30, 28, 26, 55, 61,\n",
              "          8, 52, 25, 32, 47, 11, 13, 54,  2, 62, 49, 12, 59, 40, 63, 10, 41, 57,\n",
              "         51,  5, 20, 38, 19, 18,  0, 50, 29, 45, 16, 21, 58,  7, 24, 46, 31, 35,\n",
              "         44,  1, 23, 14, 60, 53,  6, 22, 43, 36]))]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "class TranslationSet:\n",
        "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "    self.data = df\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    selected_row = self.data.iloc[idx]\n",
        "    source = selected_row['원문']\n",
        "    target = selected_row['번역문']\n",
        "\n",
        "    source_enc = self.src_tokenizer(source)['input_ids']\n",
        "    target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "    return torch.LongTensor(source_enc), torch.LongTensor(target_enc[:-1]), torch.LongTensor(target_enc[1:])\n",
        "\n",
        "entireset = TranslationSet(df, src_tokenizer, tgt_tokenizer)\n",
        "trainset, validset, testset = torch.utils.data.random_split(entireset, [int(len(entireset)*0.9), int(len(entireset)*0.05), len(entireset)-int(len(entireset)*0.9)-int(len(entireset)*0.05)], generator=torch.Generator().manual_seed(42))\n",
        "# trainset, validset, testset = torch.utils.data.random_split(entireset, [360000, 20000, 20000], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(f'Dataset Item Example: {entireset[0]}')\n",
        "print(f'Length of split : Train {len(trainset)}, Valid {len(validset)}, Test {len(testset)}')\n",
        "\n",
        "def pack_collate(raw_batch):\n",
        "  source, target, shifted_target = zip(*raw_batch)\n",
        "  return pack_sequence(source, enforce_sorted=False), pack_sequence(target, enforce_sorted=False), pack_sequence(shifted_target, enforce_sorted=False)\n",
        "\n",
        "single_loader = DataLoader(trainset, batch_size=1, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "train_loader = DataLoader(trainset, batch_size=64, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a254bfb6",
      "metadata": {
        "id": "a254bfb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f661a42b-b610-47b4-a62e-d89b23471ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Pre-defined class\n",
        "'''\n",
        "NUM_IMG = 100 # This will define how many images you will download\n",
        "\n",
        "try: # check whether you are running the notebook on colab\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  from google.colab import drive # if you selected to save it in your Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device, save_dir='./'):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "\n",
        "    self.model.to(device)\n",
        "\n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "\n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "    self.save_dir = Path(save_dir)\n",
        "    self.save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "  def save_model(self, path='kor_eng_translator_attention_model.pt'):\n",
        "    save_path = self.save_dir / path\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, save_path)\n",
        "\n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      with tqdm(self.train_loader, leave=False) as pbar:\n",
        "        for batch in pbar:\n",
        "          loss_value = self._train_by_single_batch(batch)\n",
        "          self.training_loss.append(loss_value)\n",
        "          pbar.set_description(f\"Epoch {epoch+1}, Loss {loss_value:.4f}\")\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "\n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('kor_eng_translator_attention_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('kor_eng_translator_attention_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "\n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use variables below:\n",
        "\n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "    '''\n",
        "    src, tgt, shifted_tgt = batch\n",
        "    src = src.to(self.device)\n",
        "    tgt = tgt.to(self.device)\n",
        "    shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "    prob = self.model(src, tgt)\n",
        "\n",
        "    if isinstance(prob, PackedSequence):\n",
        "      loss = self.loss_fn(prob.data, shifted_tgt.data)\n",
        "    else:\n",
        "      loss = self.loss_fn(prob, shifted_tgt)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "\n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "\n",
        "\n",
        "    output:\n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "    '''\n",
        "\n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    validation_loss = 0\n",
        "    num_correct_guess = 0\n",
        "    num_data = 0\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(loader, leave=False):\n",
        "        src, tgt, shifted_tgt = batch\n",
        "        src = src.to(self.device)\n",
        "        tgt = tgt.to(self.device)\n",
        "        shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "        prob = self.model(src, tgt)\n",
        "\n",
        "        if isinstance(prob, PackedSequence):\n",
        "          loss = self.loss_fn(prob.data, shifted_tgt.data)\n",
        "        else:\n",
        "          loss = self.loss_fn(prob, shifted_tgt)\n",
        "\n",
        "        validation_loss += loss.item() * len(prob.data)\n",
        "        if isinstance(prob, PackedSequence):\n",
        "          num_correct_guess += (prob.data.argmax(dim=-1) == shifted_tgt.data).sum().item()\n",
        "        else:\n",
        "          num_correct_guess += (prob.argmax(dim=-1) == shifted_tgt.data).sum().item()\n",
        "        num_data += len(prob.data)\n",
        "    return validation_loss / num_data, num_correct_guess / num_data\n",
        "\n",
        "\n",
        "def nll_loss(pred, target, eps=1e-8):\n",
        "  '''\n",
        "  for PackedSequence, the input is 2D tensor\n",
        "\n",
        "  predicted_prob_distribution has a shape of [num_entire_tokens_in_the_batch x vocab_size]\n",
        "  indices_of_correct_token has a shape of [num_entire_tokens_in_the_batch]\n",
        "  '''\n",
        "\n",
        "  if pred.ndim == 3:\n",
        "    pred = pred.flatten(0, 1)\n",
        "  if target.ndim == 2:\n",
        "    target = target.flatten(0, 1)\n",
        "  assert pred.ndim == 2\n",
        "  assert target.ndim == 1\n",
        "  return -torch.log(pred[torch.arange(len(target)), target] + eps).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "11a011e3",
      "metadata": {
        "id": "11a011e3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Pre-defined class\n",
        "\n",
        "You don't need to change this code\n",
        "'''\n",
        "class TranslatorBi(nn.Module):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=256, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    self.src_vocab_size = self.src_tokenizer.vocab_size\n",
        "    self.tgt_vocab_size = self.tgt_tokenizer.vocab_size\n",
        "\n",
        "    self.src_embedder = nn.Embedding(self.src_vocab_size, hidden_size)\n",
        "    self.tgt_embedder = nn.Embedding(self.tgt_vocab_size, hidden_size)\n",
        "\n",
        "    self.encoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "    self.decoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    self.decoder_proj = nn.Linear(hidden_size, self.tgt_vocab_size)\n",
        "\n",
        "  def run_encoder(self, x):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb_x = PackedSequence(self.src_embedder(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    else:\n",
        "      emb_x = self.src_embedder(x)\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden = self.encoder(emb_x)\n",
        "\n",
        "    # Because we use bi-directional GRU, there are (num_layers * 2) last hidden states\n",
        "    # Here, we make it to (num_layers) last hidden states by taking mean of [left-to-right-GRU] and [right-to-left-GRU]\n",
        "    last_hidden_sum = last_hidden.reshape(self.encoder.num_layers, 2, last_hidden.shape[1], -1).mean(dim=1)\n",
        "    if isinstance(x, PackedSequence):\n",
        "      hidden_mean = enc_hidden_state_by_t.data.reshape(-1, 2, last_hidden_sum.shape[-1]).mean(1)\n",
        "      enc_hidden_state_by_t = PackedSequence(hidden_mean, x[1], x[2], x[3])\n",
        "    else:\n",
        "      enc_hidden_state_by_t = enc_hidden_state_by_t.reshape(x.shape[0], x.shape[1], 2, -1).mean(dim=2)\n",
        "\n",
        "\n",
        "    return enc_hidden_state_by_t, last_hidden_sum\n",
        "\n",
        "  def run_decoder(self, y, last_hidden_state):\n",
        "    if isinstance(y, PackedSequence):\n",
        "      emb_y = PackedSequence(self.tgt_embedder(y.data), batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      emb_y = self.tgt_embedder(y)\n",
        "    out, decoder_last_hidden = self.decoder(emb_y, last_hidden_state)\n",
        "    return out, decoder_last_hidden\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "    y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    '''\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    out, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if isinstance(out, PackedSequence):\n",
        "      logits = self.decoder_proj(out.data)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      probs = PackedSequence(probs, batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      logits = self.decoder_proj(out)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2060d164",
      "metadata": {
        "id": "2060d164"
      },
      "source": [
        "### Problem 3.1: Complete the Seq2Seq with Attention (8 pts)\n",
        "- **Caution**: You have to concatenate [decoder_hidden_state; attention_out] for this implementation\n",
        "    - You can use different order of concatenation, but the pre-trained model used that specific order, so please follow it so that you can use the pre-trained weight correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9e285f77",
      "metadata": {
        "id": "9e285f77",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9c231d-4d18-46f3-c617-e435e3a7e5b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[ 0.0385,  0.0021,  0.1027,  ..., -0.0330, -0.2281,  0.1407],\n",
              "        [ 0.0527, -0.0065,  0.0762,  ..., -0.0574, -0.1954,  0.1672],\n",
              "        [ 0.0449, -0.0053,  0.0625,  ..., -0.0347, -0.2093,  0.1956],\n",
              "        ...,\n",
              "        [ 0.0399, -0.1248,  0.0502,  ...,  0.0542, -0.2272, -0.0025],\n",
              "        [ 0.1025, -0.0770,  0.1819,  ..., -0.0435, -0.1288,  0.0122],\n",
              "        [ 0.0969, -0.0971,  0.1644,  ..., -0.0144, -0.1672, -0.0226]],\n",
              "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 62, 61, 58, 55, 54, 52, 49, 48, 45, 43,\n",
              "        41, 38, 35, 33, 33, 33, 33, 32, 32, 31, 30, 30, 30, 29, 28, 27, 24, 23,\n",
              "        19, 18, 17, 16, 14, 13, 11, 11, 10, 10,  9,  7,  7,  5,  5,  5,  5,  4,\n",
              "         4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "         2,  2,  2,  2,  2,  2,  2,  2,  1,  1]), sorted_indices=tensor([42, 55, 26,  7,  1, 37, 60, 49, 18,  0, 33, 23, 29, 24, 57,  6, 46,  9,\n",
              "        41, 40, 38, 47, 61, 56, 50, 20, 15,  4, 14, 44, 13, 52, 21,  2, 11, 53,\n",
              "        63, 10, 39,  5, 31, 34,  8, 62, 54, 45, 51, 22,  3, 28, 43, 36, 19, 59,\n",
              "        25, 16, 12, 35, 48, 30, 58, 17, 27, 32]), unsorted_indices=tensor([ 9,  4, 33, 48, 27, 39, 15,  3, 42, 17, 37, 34, 56, 30, 28, 26, 55, 61,\n",
              "         8, 52, 25, 32, 47, 11, 13, 54,  2, 62, 49, 12, 59, 40, 63, 10, 41, 57,\n",
              "        51,  5, 20, 38, 19, 18,  0, 50, 29, 45, 16, 21, 58,  7, 24, 46, 31, 35,\n",
              "        44,  1, 23, 14, 60, 53,  6, 22, 43, 36]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "class TranslatorAtt(TranslatorBi):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=512, num_layers=3):\n",
        "    super().__init__(src_tokenizer, tgt_tokenizer, hidden_size, num_layers)\n",
        "\n",
        "    # define new self.decoder_proj\n",
        "    self.decoder_proj = nn.Linear(hidden_size * 2, self.tgt_vocab_size)\n",
        "\n",
        "  def get_attention_vector(self, encoder_hidden_states, decoder_hidden_states, mask):\n",
        "    '''\n",
        "    Arguments:\n",
        "      encoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of encoder GRU. Shape: [N, Ts, C]\n",
        "      decoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of decoder GRU. Shape: [N, Tt, C]\n",
        "      mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Ts]\n",
        "\n",
        "    Outputs:\n",
        "      attention_vectors (torch.Tensor or PackedSequence): Attention vectors that has the same shape as decoder_hidden_states\n",
        "      attention_weights (torch.Tensor): Zero-padded attention weights.\n",
        "                                You don't need to return it during the training, but it will help you to implement later problem\n",
        "\n",
        "    TODO: Complete this function using following functions\n",
        "      get_attention_score_for_a_batch_multiple_query\n",
        "      get_masked_softmax\n",
        "      get_batch_weighted_sum\n",
        "    If the inputs are PackedSequence, the output has to be a PackedSequence\n",
        "    Use torch.nn.utils.rnn.pad_packed_sequence(packed_sequence, batch_first=True) to convert PackedSequence to Tensor\n",
        "    Use torch.nn.utils.rnn.pack_padded_sequence(tensor, batch_lens, batch_first=True) to convert Tensor to PackedSequence\n",
        "    '''\n",
        "    is_packed = isinstance(encoder_hidden_states, PackedSequence)\n",
        "    if is_packed:\n",
        "      encoder_hidden_states, source_lens = pad_packed_sequence(encoder_hidden_states, batch_first=True)\n",
        "      decoder_hidden_states, target_lens = pad_packed_sequence(decoder_hidden_states, batch_first=True)\n",
        "\n",
        "    # Write your code from here\n",
        "\n",
        "    # 1. Calculate attention score using encoder_hidden_states and decoder_hidden_states\n",
        "    # 2. Mask the attention score using mask and apply softmax to get attention weight\n",
        "    # 3. Calculate attention vector using attention weight and encoder_hidden_states\n",
        "\n",
        " # -------------------------------------------------------------------\n",
        "    # TODO: 코드를 여기서부터 작성하세요.\n",
        "    # 1. encoder_hidden_states (keys)와 decoder_hidden_states (queries)를 사용하여 어텐션 스코어를 계산합니다.\n",
        "    #    (get_attention_score_for_a_batch_multiple_query 함수 사용)\n",
        "    #    keys: [N, Ts, C], queries: [N, Tt, C] -> attention_score: [N, Tt, Ts]\n",
        "    attention_scores = get_attention_score_for_a_batch_multiple_query(encoder_hidden_states, decoder_hidden_states)\n",
        "\n",
        "    # 2. attention_score에 마스크를 적용하고 소프트맥스를 사용하여 어텐션 가중치를 얻습니다.\n",
        "    #    (get_masked_softmax 함수 사용)\n",
        "    #    attention_score: [N, Tt, Ts], mask: [N, Ts] -> attention_weights: [N, Tt, Ts]\n",
        "    attention_weights = get_masked_softmax(attention_scores, mask)\n",
        "\n",
        "    # 3. attention_weights와 encoder_hidden_states (values)를 사용하여 어텐션 벡터를 계산합니다.\n",
        "    #    (get_batch_weighted_sum 함수 사용)\n",
        "    #    values: [N, Ts, C], attention_weights: [N, Tt, Ts] -> attention_vectors: [N, Tt, C]\n",
        "    #    여기서 values는 encoder_hidden_states가 됩니다.\n",
        "    attention_vectors = get_batch_weighted_sum(encoder_hidden_states, attention_weights)\n",
        "    # -------------------------------------------------------------------\n",
        "\n",
        "    # 출력이 PackedSequence여야 한다면 다시 PackedSequence로 변환합니다.\n",
        "    # 이 부분은 NLP_Assignment_4.ipynb에 이미 제공된 코드입니다.\n",
        "    if is_packed:\n",
        "        # attention_vectors는 [N, Tt, C] 형태이므로, target_lens를 사용하여 패킹합니다.\n",
        "        attention_vectors = pack_padded_sequence(attention_vectors, target_lens, batch_first=True, enforce_sorted=False)\n",
        "        # attention_weights는 [N, Tt, Ts] 형태이지만, 문제 조건상 PackedSequence로 반환할 필요는 없다고 되어 있습니다.\n",
        "        # 하지만 일관성을 위해 PackedSequence 형태로 반환해야 한다면, 이 부분도 pack_padded_sequence를 적용해야 합니다.\n",
        "        # 현재는 그대로 Tensor 형태로 반환하도록 가정합니다. (혹은 별도의 길이 정보 없이 반환)\n",
        "        # 만약 PackedSequence로 반환해야 한다면, attention_weights = pack_padded_sequence(attention_weights, target_lens, batch_first=True)\n",
        "        pass # attention_weights는 그대로 Tensor 형태로 반환 (혹은 필요시 별도 처리)\n",
        "\n",
        "    return attention_vectors, attention_weights\n",
        "\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    Arguments:\n",
        "      x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "      y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    Output:\n",
        "      prob_dist (torch.Tensor or PackedSequence): Batch of probability distribution of word for target sentence\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "\n",
        "    is_packed = isinstance(x, PackedSequence)\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    dec_hidden_state_by_t, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if is_packed:\n",
        "      mask = pad_packed_sequence(x, batch_first=True)[0] != 0\n",
        "    else:\n",
        "      mask = torch.ones(x.shape[0], x.shape[1])\n",
        "\n",
        "    # TODO: Write your code from here\n",
        "    # CAUTION:\n",
        "    #   For the concatenation, you have to concat [dec_hidden_state_by_t; attention_vec], not [attention_vec; dec_hidden_state_by_t]\n",
        "# -------------------------------------------------------------------\n",
        "    # TODO: 코드를 여기서부터 작성하세요.\n",
        "\n",
        "    # 1. get_attention_vector 함수를 호출하여 어텐션 벡터와 어텐션 가중치를 얻습니다.\n",
        "    # attention_vec: [N, Tt, C] (PackedSequence 또는 Tensor)\n",
        "    # attention_weight: [N, Tt, Ts] (Tensor)\n",
        "    attention_vec, attention_weight = self.get_attention_vector(enc_hidden_state_by_t, dec_hidden_state_by_t, mask)\n",
        "\n",
        "    # PackedSequence일 경우, 디코더 은닉 상태와 어텐션 벡터를 텐서로 변환합니다.\n",
        "    # self.decoder_proj는 PackedSequence를 직접 처리하지 못하므로, Tensor로 변환해야 합니다.\n",
        "    # 어텐션 벡터는 get_attention_vector에서 이미 PackedSequence일 경우 처리되므로,\n",
        "    # 여기서는 decoder_hidden_states만 추가로 패딩 해제합니다.\n",
        "    if is_packed:\n",
        "        # dec_hidden_state_by_t가 PackedSequence일 수 있으므로 텐서로 변환합니다.\n",
        "        # attention_vec은 get_attention_vector 내부에서 PackedSequence로 다시 변환되었을 수 있으므로\n",
        "        # 이곳에서도 다시 Tensor로 변환하여 concatenate 준비합니다.\n",
        "        dec_hidden_state_by_t_unpacked, _ = pad_packed_sequence(dec_hidden_state_by_t, batch_first=True)\n",
        "        attention_vec_unpacked, original_target_lens = pad_packed_sequence(attention_vec, batch_first=True)\n",
        "        # 중요: original_target_lens는 y의 실제 길이를 나타냅니다.\n",
        "        # 이 길이는 최종 prob_dist를 다시 PackedSequence로 패킹할 때 사용됩니다.\n",
        "    else:\n",
        "        # is_packed가 False인 경우, dec_hidden_state_by_t와 attention_vec은 이미 Tensor입니다.\n",
        "        dec_hidden_state_by_t_unpacked = dec_hidden_state_by_t\n",
        "        attention_vec_unpacked = attention_vec\n",
        "        # Tensor 입력의 경우, 길이는 해당 텐서의 실제 길이가 됩니다.\n",
        "        # 여기서는 단순히 y의 두 번째 차원(시퀀스 길이)을 사용합니다.\n",
        "        original_target_lens = torch.full((y.shape[0],), y.shape[1], dtype=torch.long, device=y.device)\n",
        "\n",
        "\n",
        "    # 2. 디코더 은닉 상태와 어텐션 벡터를 연결합니다. (Concatenation)\n",
        "    # CAUTION: [dec_hidden_state_by_t; attention_vec] 순서로 연결해야 합니다.\n",
        "    # 두 텐서의 모양은 [N, Tt, C]로 동일합니다.\n",
        "    # torch.cat(tensors, dim)은 지정된 차원을 따라 텐서들을 연결합니다.\n",
        "    # 여기서는 마지막 차원(C)을 따라 연결하여 [N, Tt, 2*C] 형태를 만듭니다.\n",
        "    combined_output = torch.cat((dec_hidden_state_by_t_unpacked, attention_vec_unpacked), dim=-1) # [N, Tt, C*2]\n",
        "\n",
        "    # 3. 연결된 벡터를 self.decoder_proj (최종 출력 계층)에 통과시켜 확률 분포를 얻습니다.\n",
        "    # self.decoder_proj는 [N, Tt, C*2]를 받아 [N, Tt, tgt_vocab_size]를 반환합니다.\n",
        "    prob_dist = self.decoder_proj(combined_output)\n",
        "\n",
        "    # 4. PackedSequence였을 경우 다시 PackedSequence로 패킹합니다.\n",
        "    # 이 부분은 NLP_Assignment_4.ipynb에 이미 제공된 코드입니다.\n",
        "    if is_packed:\n",
        "        # original_target_lens는 `if is_packed:` 블록의 첫 부분에서 `pad_packed_sequence(y, batch_first=True)`\n",
        "        # 를 호출하여 얻는 것이 가장 정확합니다.\n",
        "        # `dec_hidden_state_by_t_unpacked, original_target_lens = pad_packed_sequence(y, batch_first=True)`\n",
        "        # 이전에 `decoder_last_hidden`을 얻을 때 `y`가 PackedSequence로 유지될 수 있으므로,\n",
        "        # `original_target_lens`를 이 위치에서 다시 정확히 얻는 것이 좋습니다.\n",
        "        # 또는 `get_attention_vector`에서 반환된 `target_lens`를 사용하는 것도 한 방법입니다.\n",
        "        # 여기서는 `y`가 `PackedSequence`임을 가정하고 `pad_packed_sequence`를 다시 호출하여 길이를 얻습니다.\n",
        "        _, original_target_lens = pad_packed_sequence(y, batch_first=True)\n",
        "        prob_dist = pack_padded_sequence(prob_dist, original_target_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    return prob_dist\n",
        "\n",
        "\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, hidden_size=32, num_layers=2)\n",
        "\n",
        "model(batch[0], batch[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3212344b",
      "metadata": {
        "id": "3212344b"
      },
      "source": [
        "#### Test your model\n",
        "- To evaluate your implementation, you have to load the pretrained weight of the same model.\n",
        "- If your implementation is correct, the resulting value would be the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2d4cba9e",
      "metadata": {
        "id": "2d4cba9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35c28ec-9ac9-4aab-a536-100fdd691867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1IpqJ6U1fuIUf1b8fuwHPSnp7pMU6Qzl4\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1ch3LdtNprU5QBteP_UJLHaBP9jasUbc3\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ],
      "source": [
        "# Download pre-trained model and tensor data\n",
        "!gdown 1IpqJ6U1fuIUf1b8fuwHPSnp7pMU6Qzl4\n",
        "!gdown 1ch3LdtNprU5QBteP_UJLHaBP9jasUbc3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/"
      ],
      "metadata": {
        "id": "wHyT0nhTKkgT",
        "outputId": "e9d3860f-9570-4791-ebf8-d2e60bbf5787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wHyT0nhTKkgT",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1375680\n",
            "-rw-r--r-- 1 root root     13767 Jun 27 11:42 assignment4_pre_defined.py\n",
            "-rw-r--r-- 1 root root 266807867 Jun 27 11:47 assignment4_values.pt\n",
            "drwx------ 5 root root      4096 Jun 27 11:49 drive\n",
            "drwxr-xr-x 2 root root      4096 May 21  2023 hugging_eng_32000\n",
            "drwxr-xr-x 2 root root      4096 May 21  2023 hugging_kor_32000\n",
            "-rw-r--r-- 1 root root 491782144 Jun 27 11:49 kor_eng_translator_attention_model_best.pt\n",
            "-rw-r--r-- 1 root root 459917305 May 21  2023 nia_korean_english.csv\n",
            "-rw-r--r-- 1 root root 190090968 Jun 27 11:46 nia_korean_english_csv.zip\n",
            "-rw-r--r-- 1 root root     41934 Jun 27 11:42 NLP_Assignment_4.py\n",
            "drwxr-xr-x 1 root root      4096 Jun 25 13:36 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/kor_eng_translator_attention_model_best.pt'\n",
        "values_path = '/content/assignment4_values.pt'\n",
        "\n",
        "# Load pretrained weight\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "\n",
        "# map_location을 사용하여 모델을 현재 디바이스(CPU 또는 CUDA)로 로드\n",
        "# 또한, state_dict 로드 시에 strict=False를 추가하여\n",
        "# 혹시라도 state_dict의 키 이름이 정확히 일치하지 않는 경우를 대비할 수 있습니다.\n",
        "# (예: 이전 버전과의 호환성 문제)\n",
        "try:\n",
        "    state_dict = torch.load(model_path, map_location=device)['model']\n",
        "    model.load_state_dict(state_dict, strict=True) # strict=True로 정확히 일치하는지 확인\n",
        "    print(f\"Model loaded successfully from {model_path} to {device}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model from {model_path}: {e}\")\n",
        "    print(\"Trying to load with strict=False (may ignore some key mismatches)...\")\n",
        "    try:\n",
        "        state_dict = torch.load(model_path, map_location=device)['model']\n",
        "        model.load_state_dict(state_dict, strict=False) # strict=False로 다시 시도\n",
        "        print(f\"Model loaded successfully with strict=False from {model_path} to {device}.\")\n",
        "    except Exception as e_non_strict:\n",
        "        print(f\"Error loading model even with strict=False: {e_non_strict}\")\n",
        "        print(\"Please ensure the model file is not corrupted and matches the model architecture.\")\n",
        "\n",
        "model.to(device) # 모델을 최종적으로 올바른 디바이스로 이동\n",
        "model.eval() # 모델을 평가 모드로 전환\n",
        "\n",
        "# Load the pre-calculated example and result\n",
        "try:\n",
        "    prob3_values = torch.load(values_path, map_location=device) # values도 디바이스로 로드\n",
        "    single_batch_example = prob3_values['single_test_batch']\n",
        "    packed_batch_example = prob3_values['packed_test_batch']\n",
        "    correct_single_out = prob3_values['correct_single_out']\n",
        "    correct_packed_out = prob3_values['correct_packed_out']\n",
        "    print(f\"Test values loaded successfully from {values_path} to {device}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading test values from {values_path}: {e}\")\n",
        "    print(\"Please ensure 'assignment4_values.pt' is not corrupted.\")\n",
        "\n",
        "\n",
        "# 이후의 테스트 코드들 (`single_out = model(single_batch_example[0], single_batch_example[1])` 등)은\n",
        "# 이 수정된 로드 코드 다음에 위치해야 합니다.\n",
        "\n",
        "print(\"\\nPretrained weights and test data loading section completed.\")\n"
      ],
      "metadata": {
        "id": "Xxn61wA-LBSo",
        "outputId": "4b28e6e8-8698-4e13-ea1d-af09d49d6471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "id": "Xxn61wA-LBSo",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model from /content/kor_eng_translator_attention_model_best.pt: name 'device' is not defined\n",
            "Trying to load with strict=False (may ignore some key mismatches)...\n",
            "Error loading model even with strict=False: name 'device' is not defined\n",
            "Please ensure the model file is not corrupted and matches the model architecture.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-40-3002180931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please ensure the model file is not corrupted and matches the model architecture.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 모델을 최종적으로 올바른 디바이스로 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 모델을 평가 모드로 전환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3b291ec1",
      "metadata": {
        "id": "3b291ec1"
      },
      "outputs": [],
      "source": [
        "# Load pretrained weight\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "state_dict = torch.load('kor_eng_translator_attention_model_best.pt', map_location='cpu')['model']\n",
        "model.eval()\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Load the pre-calculated example and result\n",
        "prob3_values = torch.load('assignment4_values.pt', weights_only=False)\n",
        "single_batch_example, packed_batch_example, correct_single_out, correct_packed_out = prob3_values['single_test_batch'], prob3_values['packed_test_batch'], prob3_values['correct_single_out'],  prob3_values['correct_packed_out']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6051c11f",
      "metadata": {
        "id": "6051c11f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "bb26ce56-c354-405a-808c-087b5e8e6304"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "The output value is different from the expected",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-4022133430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The output of model for Tensor has to be Tensor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_single_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The output value is different from the expected\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: The output value is different from the expected"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test Case for Single-size Batch\n",
        "'''\n",
        "single_out = model(single_batch_example[0], single_batch_example[1])\n",
        "\n",
        "assert isinstance(single_out, torch.Tensor), \"The output of model for Tensor has to be Tensor\"\n",
        "assert torch.allclose(single_out, correct_single_out, atol=1e-4), \"The output value is different from the expected\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8027b42c",
      "metadata": {
        "id": "8027b42c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "0e1f5c4c-ef0a-434d-c3c9-a967f813d659"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "The output value is different from the expected",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-219721325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpacked_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_indices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcorrect_packed_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Output's sorted_indices is wrong\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_packed_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"The output value is different from the expected\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: The output value is different from the expected"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test Case for Batch with PackedSequence\n",
        "'''\n",
        "packed_out = model(packed_batch_example[0], packed_batch_example[1])\n",
        "\n",
        "assert isinstance(packed_out, PackedSequence), \"The output of model for PackedSequence has to be PackedSequence\"\n",
        "assert (packed_out.batch_sizes == correct_packed_out.batch_sizes).all(), \"Output's batch_sizes is wrong\"\n",
        "assert (packed_out.sorted_indices == correct_packed_out.sorted_indices).all(), \"Output's sorted_indices is wrong\"\n",
        "\n",
        "assert torch.allclose(packed_out.data, correct_packed_out.data, atol=1e-4),  \"The output value is different from the expected\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e0ab23",
      "metadata": {
        "id": "12e0ab23"
      },
      "source": [
        "### Train the model (Optional)\n",
        "- You can try to train your model, but you can just load the pretrained data\n",
        "- You don't have to train the model yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "75d4bf8b",
      "metadata": {
        "id": "75d4bf8b"
      },
      "outputs": [],
      "source": [
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "save_dir = '/drive/MyDrive/NLP_Assignment4/' if IN_COLAB else './'\n",
        "\n",
        "trainer = Trainer(model, optimizer, nll_loss, train_loader, valid_loader, 'cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "najCuqk2WIbQ",
      "metadata": {
        "id": "najCuqk2WIbQ",
        "outputId": "a8fd12a1-14cb-446b-9cb3-348f982e31ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TranslatorAtt(\n",
              "  (src_embedder): Embedding(32000, 512)\n",
              "  (tgt_embedder): Embedding(32000, 512)\n",
              "  (encoder): GRU(512, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
              "  (decoder): GRU(512, 512, num_layers=3, batch_first=True)\n",
              "  (decoder_proj): Linear(in_features=1024, out_features=32000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Otherwise, you have to load the model weight\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "state_dict = torch.load('kor_eng_translator_attention_model_best.pt', map_location='cpu')['model']\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea22364",
      "metadata": {
        "id": "cea22364"
      },
      "source": [
        "### Problem 3.2: Implement Inference with Attention Weights (6 pts)\n",
        "- In this problem, you have to implement an inference code that returns translation for given source sentence, but also **attention weights** between source sentence and target sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef0e100",
      "metadata": {
        "id": "1ef0e100"
      },
      "outputs": [],
      "source": [
        "def translate(model, source_sentence):\n",
        "  '''\n",
        "  This function translates a given sentence using a given model.\n",
        "  It returns the tokenized source sentence, tokenized translated sentence, translated sentence in string, and attention map\n",
        "\n",
        "  Arguments:\n",
        "    model (TranslatorAtt): Translator model with attention\n",
        "    source_sentence (str): Sentence to translate\n",
        "\n",
        "  Returns:\n",
        "    input_tokens (list): Source sentence in a list of token in token_id\n",
        "    predicted_tokens (list): Translated sentence in a list of token in token_id\n",
        "    decoded_string (str): Translated sentence in string\n",
        "    attention_map (torch.Tensor): Attention weight between each token of source sentence and target sentence. Has a shape of [Tt, Ts]\n",
        "\n",
        "  '''\n",
        "\n",
        "  input_tokens = model.src_tokenizer.encode(source_sentence)\n",
        "  input_tensor = torch.LongTensor(input_tokens).unsqueeze(0)\n",
        "  mask = torch.ones_like(input_tensor)\n",
        "  enc_hidden_state_by_t, last_hidden_sum = model.run_encoder(input_tensor)\n",
        "\n",
        "  # Setup for 0th step\n",
        "  current_hidden = last_hidden_sum\n",
        "  current_decoder_token = torch.LongTensor([[2]]) # start of sentence token\n",
        "  total_output = []\n",
        "  total_attetion_weights = []\n",
        "\n",
        "  for i in range(100): # You can chage it to while True:\n",
        "    emb = model.tgt_embedder(current_decoder_token)\n",
        "    '''\n",
        "    TODO: Complete the code here\n",
        "\n",
        "    You have to\n",
        "      1) run decoder rnn for a single step\n",
        "      2) get attention weight (variable name: att_weight) and attention vector.\n",
        "         att_weight.shape == torch.Size([1, 1, len(tokenized_sentence)])\n",
        "      3) concat decoder out and attention vector\n",
        "      4) calculate probabilty logit (variable name: logit)\n",
        "    '''\n",
        "    dec_output_one_step, last_hidden = model.decoder_gru(emb, current_hidden)\n",
        "    att_vec, att_weight = model.get_attention_vector(\n",
        "        encoder_hidden_states_unpacked.unsqueeze(0) if encoder_hidden_states_unpacked.ndim == 2 else encoder_hidden_states_unpacked, # N=1 차원 추가\n",
        "        dec_output_one_step, # [1, 1, C]\n",
        "        mask # [1, Ts]\n",
        "    )\n",
        "    combined_output = torch.cat((dec_output_one_step, att_vec), dim=-1)\n",
        "    logit = model.decoder_proj(combined_output) # [1, 1, vocab_size]\n",
        "\n",
        "\n",
        "    # You don't have to change the codes below.\n",
        "    # Declare logit and last_hidden properly so that the code below can run without error\n",
        "    selected_token = torch.argmax(logit, dim=-1)\n",
        "    current_decoder_token = selected_token\n",
        "    current_hidden = last_hidden\n",
        "    if current_decoder_token == 3: ## end of sentence token\n",
        "      break\n",
        "    total_output.append(selected_token[0])\n",
        "    total_attetion_weights.append(att_weight[0,0])\n",
        "  predicted_tokens = torch.cat(total_output, dim=0).tolist()\n",
        "  attention_map = torch.stack(total_attetion_weights, dim=1)\n",
        "\n",
        "  return  input_tokens, predicted_tokens, model.tgt_tokenizer.decode(predicted_tokens), attention_map\n",
        "\n",
        "model.cpu()\n",
        "test_sentence = '이 알고리즘을 사용하면 한국어 단어와 영어 단어가 어떻게 연결되는지를 알 수 있습니다.'\n",
        "input_tokens, pred_tokens, translated_string, att_weights  = translate(model, test_sentence)\n",
        "print(translated_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491f62ca",
      "metadata": {
        "id": "491f62ca"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test Case: This has to be run with the provided pre-trained weight, which canbe downloaded by the line below\n",
        "'''\n",
        "test_sentence = '이 알고리즘을 사용하면 한국어 단어와 영어 단어가 어떻게 연결되는지를 알 수 있습니다.'\n",
        "input_tokens, pred_tokens, translated_string, att_weights  = translate(model, test_sentence)\n",
        "\n",
        "correct_output = 'using this algorithm, you can see how korean words and english words are connected.'\n",
        "answer = torch.tensor([1.6955e-07, 1.1118e-07, 6.0198e-10, 9.1661e-01, 7.8413e-18, 1.2012e-17,\n",
        "        7.6764e-29, 1.2549e-20, 5.7431e-19, 3.3563e-31, 1.1225e-21, 1.3192e-27,\n",
        "        3.6772e-26, 2.2244e-23, 3.6098e-20, 1.5309e-21, 7.5093e-05])\n",
        "answer2 = torch.tensor([1.1643e-11, 3.9906e-30, 1.2813e-33, 2.7519e-13, 2.3483e-15, 4.2758e-12,\n",
        "        1.9385e-18, 6.8541e-16, 4.9662e-18, 5.0304e-33, 7.2299e-26, 4.4580e-25,\n",
        "        3.7096e-23, 7.5614e-22, 4.5226e-22, 2.3576e-25, 1.7577e-12])\n",
        "answer3 = torch.tensor([1.2012e-17, 1.7528e-21, 1.1316e-18, 2.7204e-17, 8.2384e-10, 3.4510e-11,\n",
        "        1.8289e-09, 1.1806e-12, 3.9218e-19, 2.8321e-16, 1.3933e-12, 3.6876e-10,\n",
        "        4.1782e-07, 1.0905e-02, 9.8673e-01, 2.3651e-03, 1.7207e-08, 4.2758e-12])\n",
        "\n",
        "\n",
        "assert translated_string == correct_output, 'Translated sentence is wrong'\n",
        "assert att_weights.shape == torch.Size([18, 17]), 'Attention weight has wrong shape'\n",
        "assert torch.allclose(att_weights[0], answer, rtol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_weights[-1], answer2, rtol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_weights[:,5], answer3, rtol=1e-4), 'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e466b4f4",
      "metadata": {
        "id": "e466b4f4"
      },
      "source": [
        "### Plot attention map\n",
        "- If you completed `translate()`, you can visualize the result of attention weight as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1bf81f6",
      "metadata": {
        "id": "f1bf81f6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(len(pred_tokens)*0.8, len(input_tokens)*0.8))\n",
        "x_axis_label = [model.tgt_tokenizer.decode(x) for x in pred_tokens]\n",
        "y_axis_label = [model.src_tokenizer.decode(x) for x in input_tokens]\n",
        "\n",
        "plt.imshow(att_weights.detach())\n",
        "plt.xticks(range(len(x_axis_label)), x_axis_label, fontsize=15,rotation = 45)\n",
        "plt.yticks(range(len(y_axis_label)), y_axis_label, fontsize=15)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7292462",
      "metadata": {
        "id": "d7292462"
      },
      "source": [
        "## Problem 4: Transformer and Self Attention (14 pts)\n",
        "- In this problem, you will implement the query-key-value calculation that was used for Transformer\n",
        "- Also, you have to implement self-attention and cross-attention, which are the core components of Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767c2c04",
      "metadata": {
        "id": "767c2c04"
      },
      "source": [
        "### 4-1 Implement Query-Key-Value Calculation (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35fb7765",
      "metadata": {
        "id": "35fb7765"
      },
      "outputs": [],
      "source": [
        "def get_query_key_value(input_tensor, qkv_layer):\n",
        "  '''\n",
        "  This function returns key, query, and value that is calculated by input tensor and nn_layer.\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "\n",
        "  Outputs:\n",
        "    queries (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "    keys (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "    values (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: Use torch.chunk() to split a tensor into given number of chunks\n",
        "  '''\n",
        "  return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(4, 17, 8)\n",
        "linear = nn.Linear(8, 16 * 3)\n",
        "queries, keys, values = get_query_key_value(test, linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c0cae6",
      "metadata": {
        "id": "82c0cae6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([ 0.5393,  0.0587,  0.6597, -1.1150, -0.7343,  0.3282,  0.0551,  0.0178,\n",
        "         0.4408, -0.3078,  0.3289, -0.4874,  0.2256, -0.1007, -0.4304, -0.2109])\n",
        "answer2 = torch.Tensor([ 0.8704, -0.2256,  0.6611,  0.0332, -0.5233, -0.1159,  0.1805,  0.7238,\n",
        "         0.5590,  0.7260,  1.3096,  0.2465,  1.1961,  0.1751, -0.9674,  0.6297])\n",
        "assert keys.ndim == queries.ndim == values.ndim == 3\n",
        "assert keys.shape == queries.shape == values.shape == torch.Size([4, 17, 16])\n",
        "assert not (keys==queries).any() and not (keys==values).any() and not (values==queries).any()\n",
        "assert torch.allclose(queries[2, 13], answer, atol=1e-4)\n",
        "assert torch.allclose(values[0, 3], answer2, atol=1e-4)\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7cd9187",
      "metadata": {
        "id": "f7cd9187"
      },
      "source": [
        "### 4-2 Implement 3d masked softmax (1 pts):\n",
        "  - This would be almost similar to ``get_masked_softmax()``\n",
        "  - It is common to use N x Tq x Tk attention score and mask, but we will use N x Tk x Tq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8167e8a2",
      "metadata": {
        "id": "8167e8a2"
      },
      "outputs": [],
      "source": [
        "def get_3d_masked_softmax(attention_score, mask):\n",
        "  '''\n",
        "  During the batch computation, each sequence in the batch can have different length.\n",
        "  To group them as in a single tensor, we usually pad values\n",
        "\n",
        "  Arguments:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Tq, Tk]\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, Tq, Tk] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, tq, tk] == 1 if and only if input_batch[n,tk] is not a padded value.\n",
        "                         If input_batch[n,tk] is a padded value, then mask[n,tq, tk] == 0\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
        "                                    Has a shape of [N, Tq, Tk]\n",
        "\n",
        "    attention_weight[n, t, i] has to be an attention weight of values[n, i] for queries[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Caution: Do not directly mask the attention score. use .clone() instead\n",
        "\n",
        "  '''\n",
        "  assert attention_score.ndim == mask.ndim == 3\n",
        "\n",
        "  return\n",
        "\n",
        "'''\n",
        "Don't change this codes\n",
        "'''\n",
        "torch.manual_seed(0)\n",
        "mask = torch.ones([3, 9, 9])\n",
        "mask[1, :, 2:] = 0\n",
        "mask[2, :, 7:] = 0\n",
        "att_score = torch.randn([3, 9, 9]).transpose(1,2) # Transpose here is just to match the test value for assertion\n",
        "att_score_modified = att_score.clone()\n",
        "att_score_modified[1, :, 2:] = 0\n",
        "attention_weight = get_3d_masked_softmax(att_score, mask)\n",
        "attention_weight_for_modified = get_3d_masked_softmax(att_score_modified, mask)\n",
        "attention_weight, attention_weight_for_modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6513919a",
      "metadata": {
        "id": "6513919a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "\n",
        "answer = torch.tensor([0.1348, 0.1429, 0.2938, 0.0369, 0.0748, 0.0577, 0.2591, 0.0000, 0.0000])\n",
        "\n",
        "assert attention_weight.ndim == 3\n",
        "assert torch.allclose(attention_weight[2,:, 0], answer, atol=1e-4)\n",
        "assert not torch.isnan(attention_weight).any(), \"Error: There is a nan value in attention_weight\"\n",
        "assert torch.allclose(attention_weight, attention_weight_for_modified, atol=1e-4), \"Error: The attention_weight are different even though only masked item is different\"\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddeea3c2",
      "metadata": {
        "id": "ddeea3c2"
      },
      "source": [
        "### 4-3 Implement Self-Attention (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc503bd",
      "metadata": {
        "id": "7dc503bd"
      },
      "outputs": [],
      "source": [
        "def get_self_attention(input_tensor, qkv_layer, mask_2d):\n",
        "  '''\n",
        "  This function returns output of self-attention for a given input tensor using with a given kqv_layer\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, T] that represents whether the corresponding is valid or not.\n",
        "\n",
        "  Outputs:\n",
        "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function using your completed functions of below:\n",
        "        get_query_key_value()\n",
        "        get_attention_score_for_a_batch_multiple_query()\n",
        "        get_3d_masked_softmax()\n",
        "        get_batch_weighted_sum()\n",
        "  '''\n",
        "  mask_3d = mask_2d.unsqueeze(1).repeat(1,input_tensor.shape[1],1)\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(5, 17, 8)\n",
        "linear = nn.Linear(8, 16 * 3)\n",
        "mask_2d = torch.ones([5, 17])\n",
        "mask_2d[2, 4:] = 0\n",
        "mask_2d[4, 14:] = 0\n",
        "\n",
        "att_vecs = get_self_attention(test, linear, mask_2d)\n",
        "modified_test = test.clone()\n",
        "modified_test[2, 4:] = 0\n",
        "modified_test[4, 14:] = 0\n",
        "modified_att_vecs = get_self_attention(modified_test, linear, mask_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602218f9",
      "metadata": {
        "id": "602218f9"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([-0.3925, -0.0043,  0.0343, -0.6713,  0.2388, -0.4703, -0.2195, -0.1550,\n",
        "        -0.0830, -0.4170, -0.1829,  0.3884,  0.2899,  0.1284,  0.0225, -0.5960])\n",
        "answer2 = torch.Tensor([-0.4078,  0.0173,  0.2670, -0.7959, -0.0314, -0.3455,  0.5751, -0.5806,\n",
        "        -0.3328, -0.2571, -0.4913, -0.1833,  0.6236, -0.5167,  0.3256, -0.9818])\n",
        "assert torch.allclose(att_vecs[3, 2], answer, atol=1e-4)\n",
        "assert torch.allclose(att_vecs[0, 11], answer2, atol=1e-4)\n",
        "assert torch.allclose(att_vecs[4, :14], modified_att_vecs[4, :14], atol=1e-6)\n",
        "\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b23835",
      "metadata": {
        "id": "a1b23835"
      },
      "source": [
        "### 4-4 Implement Multi-head split and concat (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837eeac1",
      "metadata": {
        "id": "837eeac1"
      },
      "outputs": [],
      "source": [
        "def get_multihead_split(x, num_head):\n",
        "  '''\n",
        "  This function returns split tensor for multi-head attention\n",
        "\n",
        "  Arguments:\n",
        "    x (torch.Tensor): Has a shape of [N, T, C]\n",
        "    num_head (int): Number of heads\n",
        "\n",
        "  Output:\n",
        "    x (torch.Tensor): Has a shape of [N * num_head, T, C // num_head]\n",
        "    The order of N * num_head is [Batch1_head1, Batch1_head2, ..., Batch1_headN, Batch2_head1, Batch2_head2, ..., Batch2_headN, ...]\n",
        "  '''\n",
        "  assert x.shape[-1] % num_head == 0\n",
        "\n",
        "  # TODO: Complete this function\n",
        "  return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "dummy_input = torch.randn(4, 17, 32)\n",
        "head_split_output = get_multihead_split(dummy_input, 8)\n",
        "head_split_output.shape\n",
        "\n",
        "assert head_split_output.shape == torch.Size([32, 17, 4])\n",
        "assert (dummy_input[0, :, 4:8] == head_split_output[1]).all()\n",
        "assert (dummy_input[0, 3, 8:12] == head_split_output[2, 3, :]).all()\n",
        "assert (dummy_input[2, 10, 16:20] == head_split_output[20, 10, :]).all()\n",
        "print('Passed all the cases!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ac7f68",
      "metadata": {
        "id": "b0ac7f68"
      },
      "outputs": [],
      "source": [
        "def get_multihead_concat(x, num_head):\n",
        "  '''\n",
        "  This function returns concat tensor for multi-head attention\n",
        "\n",
        "  Arguments:\n",
        "    x (torch.Tensor): Has a shape of [N * num_head, T, C // num_head]\n",
        "    num_head (int): Number of heads\n",
        "  Outputs:\n",
        "    x (torch.Tensor): Has a shape of [N, T, C]\n",
        "  '''\n",
        "  # TODO: Complete this function\n",
        "  assert x.shape[0] % num_head == 0\n",
        "\n",
        "  return\n",
        "\n",
        "head_cat_output = get_multihead_concat(head_split_output, 8)\n",
        "print(f\"Output shape: {head_cat_output.shape}\")\n",
        "assert head_cat_output.shape == torch.Size([4, 17, 32])\n",
        "assert (dummy_input == head_cat_output).all()\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ff032b3",
      "metadata": {
        "id": "6ff032b3"
      },
      "source": [
        "### 4-5 Implement Transformer-like multi-head self-attention (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b655a3",
      "metadata": {
        "id": "b7b655a3"
      },
      "outputs": [],
      "source": [
        "def get_multi_head_self_attention(input_tensor, qkv_layer, output_proj_layer, mask, num_head=8):\n",
        "  '''\n",
        "  This function returns output of multi-headed self-attention for a given input tensor using with a given kqv_layer\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    qkv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "    output_proj_layer (torch.nn.Linear): Linear layer with in_features=Cn and out_features=C\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, T, T] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
        "                         If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
        "    num_head (int): Number of heads\n",
        "\n",
        "  Outputs:\n",
        "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function using your completed functions of below:\n",
        "        get_query_key_value(): Get QKV from input_tensor and kqv_layer\n",
        "\n",
        "        get_multihead_split(): Split QKV into multiple heads\n",
        "\n",
        "        get_attention_score_for_a_batch_multiple_query(): Get attention score for a batch of multiple queries\n",
        "          CAUTION: You have to scale the attention score by dividing by sqrt(Cn // num_head)\n",
        "          HINT: att_score /= keys.shape[-1] ** 0.5\n",
        "\n",
        "        get_3d_masked_softmax(): Get masked softmax/\n",
        "          CAUTION: You have to repeat mask for num_head times to use it for multi-head attention\n",
        "          USE head_repeated_mask\n",
        "        get_batch_weighted_sum(): Get batch weighted sum\n",
        "\n",
        "        get_multihead_concat(): Concatenate multiple heads into a single tensor\n",
        "\n",
        "        Additionally, use output_proj_layer to project concatenated tensor at the final step\n",
        "  '''\n",
        "  head_repeated_mask = mask.unsqueeze(1).repeat(1, num_head, 1, 1).reshape(-1, mask.shape[1], mask.shape[2])\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(5, 17, 16)\n",
        "linear = nn.Linear(16, 16 * 3)\n",
        "out_proj = nn.Linear(16, 16)\n",
        "\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, :, 4:] = 0\n",
        "mask[4, :, 14:] = 0\n",
        "\n",
        "att_vecs = get_multi_head_self_attention(test, linear, out_proj, mask, num_head=4)\n",
        "att_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a3d6ac",
      "metadata": {
        "id": "c7a3d6ac"
      },
      "outputs": [],
      "source": [
        "wrong_answer = torch.tensor([ 0.0105,  0.2337,  0.1587, -0.0346,  0.1446,  0.0231,  0.0296, -0.0437,\n",
        "        -0.1101,  0.2079, -0.0813, -0.2343, -0.3270,  0.2156, -0.0427, -0.0313])\n",
        "wrong_answer2 = torch.tensor([ 0.0557,  0.1026, -0.0834,  0.1770, -0.4083, -0.1543,  0.2025, -0.1570,\n",
        "         0.0974, -0.0317,  0.0569, -0.0610,  0.0063,  0.0107, -0.0727,  0.0796])\n",
        "\n",
        "assert not torch.allclose(att_vecs[0, 0], wrong_answer, atol=1e-4), \"Error: You did not include attention score scaling. Read the CAUTION!\"\n",
        "assert not torch.allclose(att_vecs[0, 0], wrong_answer2, atol=1e-4), \"Error: You did not include output projection. Read the TODO!\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422a8e6d",
      "metadata": {
        "id": "422a8e6d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Compare with official implementation of PyTorch\n",
        "'''\n",
        "\n",
        "official_attention = torch.nn.MultiheadAttention(16, num_heads=4, batch_first=True)\n",
        "official_attention.in_proj_weight.data = linear.weight.data\n",
        "official_attention.in_proj_bias.data = linear.bias.data\n",
        "official_attention.out_proj.weight.data = out_proj.weight.data\n",
        "official_attention.out_proj.bias.data = out_proj.bias.data\n",
        "\n",
        "head_repeated_mask = mask.unsqueeze(1).repeat(1, 4, 1, 1).reshape(-1, mask.shape[1], mask.shape[2]).transpose(1,2)\n",
        "official_attention_output, attention_weights = official_attention(test, test, test, attn_mask=head_repeated_mask==0)\n",
        "\n",
        "assert torch.allclose(att_vecs, official_attention_output, atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c169acb5",
      "metadata": {
        "id": "c169acb5"
      },
      "source": [
        "### 4-6 Implement it as a single module (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48M0cSsLW8DA",
      "metadata": {
        "id": "48M0cSsLW8DA"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_head, mask_value=0):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.qkv = nn.Linear(self.input_size, self.hidden_size * 3)\n",
        "    self.out_proj = nn.Linear(self.hidden_size, self.input_size)\n",
        "    self.mask_value = mask_value\n",
        "    self.num_head = num_head\n",
        "    assert self.hidden_size % self.num_head == 0\n",
        "    self.dim_per_head = self.hidden_size // self.num_head\n",
        "\n",
        "  '''\n",
        "  TODO: Implement this function as functions you implemented above\n",
        "  '''\n",
        "  def _get_qkv(self, x):\n",
        "    return\n",
        "\n",
        "  def _get_multihead_split(self, x):\n",
        "    return\n",
        "\n",
        "  def _get_multiheaded_att_score(self, keys, queries):\n",
        "    return\n",
        "\n",
        "  def _get_masked_softmax(self, score, masks):\n",
        "    return\n",
        "\n",
        "  def _get_weighted_sum(self, values, weights):\n",
        "    return\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    '''\n",
        "    TODO: Implement this function using the functions you implemented above\n",
        "    '''\n",
        "    if mask is None:\n",
        "      mask = torch.ones([x.shape[0], x.shape[1], x.shape[1]])\n",
        "    if mask.ndim == 2:\n",
        "      # TODO: Convert mask to 3D mask\n",
        "      pass\n",
        "\n",
        "    return\n",
        "\n",
        "torch.manual_seed(0)\n",
        "attention_module = SelfAttention(512, 512, 8)\n",
        "test = torch.randn(5, 17, 512)\n",
        "mask_2d = torch.ones([5, 17])\n",
        "mask_2d[2, 4:] = 0\n",
        "mask_2d[4, 14:] = 0\n",
        "\n",
        "out = attention_module(test, mask_2d)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0ed205",
      "metadata": {
        "id": "ef0ed205"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test case\n",
        "'''\n",
        "\n",
        "official_attention = torch.nn.MultiheadAttention(512, num_heads=8, batch_first=True)\n",
        "official_attention.in_proj_weight.data = attention_module.qkv.weight.data\n",
        "official_attention.in_proj_bias.data = attention_module.qkv.bias.data\n",
        "official_attention.out_proj.weight.data = attention_module.out_proj.weight.data\n",
        "official_attention.out_proj.bias.data = attention_module.out_proj.bias.data\n",
        "\n",
        "official_attention_output, attention_weights = official_attention(test, test, test, key_padding_mask=mask_2d==0)\n",
        "\n",
        "assert torch.allclose(out, official_attention_output, atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc74bbbf",
      "metadata": {
        "id": "cc74bbbf"
      },
      "source": [
        "### Test your SelfAttention module on Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac3e753",
      "metadata": {
        "id": "bac3e753"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.input_size = in_size\n",
        "    self.layer = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(hidden_size, in_size))\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "class PosEncoding(nn.Module):\n",
        "  def __init__(self, size, max_t):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.max_t = max_t\n",
        "    self.register_buffer('encoding', self._prepare_emb())\n",
        "\n",
        "  def _prepare_emb(self):\n",
        "    dim_axis = 10000**(torch.arange(self.size//2) * 2 / self.size)\n",
        "    timesteps = torch.arange(self.max_t)\n",
        "    pos_enc_in = timesteps.unsqueeze(1) / dim_axis.unsqueeze(0)\n",
        "    pos_enc_sin = torch.sin(pos_enc_in)\n",
        "    pos_enc_cos = torch.cos(pos_enc_in)\n",
        "\n",
        "    pos_enc = torch.stack([pos_enc_sin, pos_enc_cos], dim=-1).reshape([self.max_t, 512])\n",
        "    return pos_enc\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.encoding[x]\n",
        "\n",
        "class ResidualLayerNormModule(nn.Module):\n",
        "  def __init__(self, submodule):\n",
        "    super().__init__()\n",
        "    self.submodule = submodule\n",
        "    self.layer_norm = nn.LayerNorm(self.submodule.input_size)\n",
        "\n",
        "  def forward(self, x, mask=None, y=None):\n",
        "    if y is not None:\n",
        "      res_x = self.submodule(x, y, mask)\n",
        "    elif mask is not None:\n",
        "      res_x = self.submodule(x, mask)\n",
        "    else:\n",
        "      res_x = self.submodule(x)\n",
        "    x =  x + res_x\n",
        "    return self.layer_norm(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head):\n",
        "    super().__init__()\n",
        "    self.att_block = ResidualLayerNormModule(SelfAttention(in_size, emb_size, num_head))\n",
        "    self.mlp_block = ResidualLayerNormModule(MLP(emb_size, mlp_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.mlp_block(self.att_block(x['input'], x['mask']))\n",
        "    return {'input':out, 'mask':x['mask']}\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "encoder_layer = EncoderLayer(512, 512, 2048, 8)\n",
        "test = torch.randn(5, 17, 512)\n",
        "mask_2d = torch.ones([5, 17])\n",
        "mask_2d[2, 4:] = 0\n",
        "mask_2d[4, 14:] = 0\n",
        "\n",
        "out = encoder_layer({'input':test, 'mask':mask_2d})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb07b11b",
      "metadata": {
        "id": "bb07b11b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test case\n",
        "'''\n",
        "\n",
        "official_encoder_layer = nn.TransformerEncoderLayer(512, 8, 2048, batch_first=True, dropout=0)\n",
        "\n",
        "official_encoder_layer.self_attn.in_proj_weight.data = encoder_layer.att_block.submodule.qkv.weight.data\n",
        "official_encoder_layer.self_attn.in_proj_bias.data = encoder_layer.att_block.submodule.qkv.bias.data\n",
        "official_encoder_layer.self_attn.out_proj.weight.data = encoder_layer.att_block.submodule.out_proj.weight.data\n",
        "official_encoder_layer.self_attn.out_proj.bias.data = encoder_layer.att_block.submodule.out_proj.bias.data\n",
        "official_encoder_layer.linear1.weight.data = encoder_layer.mlp_block.submodule.layer[0].weight.data\n",
        "official_encoder_layer.linear1.bias.data = encoder_layer.mlp_block.submodule.layer[0].bias.data\n",
        "official_encoder_layer.linear2.weight.data = encoder_layer.mlp_block.submodule.layer[2].weight.data\n",
        "official_encoder_layer.linear2.bias.data = encoder_layer.mlp_block.submodule.layer[2].bias.data\n",
        "official_encoder_layer.norm1.weight.data = encoder_layer.att_block.layer_norm.weight.data\n",
        "official_encoder_layer.norm1.bias.data = encoder_layer.att_block.layer_norm.bias.data\n",
        "official_encoder_layer.norm2.weight.data = encoder_layer.mlp_block.layer_norm.weight.data\n",
        "official_encoder_layer.norm2.bias.data = encoder_layer.mlp_block.layer_norm.bias.data\n",
        "\n",
        "official_encoder_output = official_encoder_layer(test, src_key_padding_mask=mask_2d==0)\n",
        "\n",
        "assert torch.allclose(official_encoder_output, out['input'], atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test case!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b169486",
      "metadata": {
        "id": "1b169486"
      },
      "source": [
        "### 4-7 Implement Transformer-like multi-head cross-attention (2 pts)\n",
        "- Implement ``CrossAttention`` inheriting `SelfAttention` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b8a722",
      "metadata": {
        "id": "b5b8a722"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(SelfAttention):\n",
        "  def __init__(self, input_size, hidden_size, num_head, mask_value=0):\n",
        "    super().__init__(input_size, hidden_size, num_head, mask_value)\n",
        "\n",
        "  def forward(self, q_seq, kv_seq, encoder_mask=None):\n",
        "    '''\n",
        "    Arguments:\n",
        "      q_seq (torch.Tensor): Sequence to be used for query\n",
        "      kv_seq (torch.Tensor): Sequence to be used for key and value\n",
        "      mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Ty]\n",
        "\n",
        "    Outs:\n",
        "      output (torch.Tensor): Output of cross attention. Shape: [N, Tx, C]\n",
        "\n",
        "    TODO: Complete this function using your completed functions of below:\n",
        "    '''\n",
        "    if encoder_mask is None:\n",
        "      encoder_mask = torch.ones([q_seq.shape[0], kv_seq.shape[1]]) # Then convert to 3D mask\n",
        "    if encoder_mask.ndim == 2:\n",
        "      # TODO: Convert mask to 3D mask\n",
        "      pass\n",
        "\n",
        "    return\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head):\n",
        "    super().__init__()\n",
        "    self.att_block = ResidualLayerNormModule(SelfAttention(in_size, emb_size, num_head))\n",
        "    self.cross_att_block = ResidualLayerNormModule(CrossAttention(in_size, emb_size, num_head))\n",
        "    self.mlp_block = ResidualLayerNormModule(MLP(emb_size, mlp_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.att_block(x['input'], x['decoder_mask'])\n",
        "    out = self.cross_att_block(out,  x['encoder_mask'], x['encoder_out'])\n",
        "    out = self.mlp_block(out)\n",
        "    return {'input':out, 'decoder_mask':x['decoder_mask'], 'encoder_out':x['encoder_out'], 'encoder_mask':x['encoder_mask']}\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "decoder_layer = DecoderLayer(512, 512, 2048, 8)\n",
        "test_src = torch.randn(5, 17, 512)\n",
        "test_tgt = torch.randn(5, 19, 512)\n",
        "mask_src = torch.ones([5, 17, 19])\n",
        "mask_src[2, 4:] = 0\n",
        "mask_src[4, 14:] = 0\n",
        "mask_tgt = torch.tril(torch.ones(test_tgt.shape[0], test_tgt.shape[1], test_tgt.shape[1]))\n",
        "\n",
        "out = decoder_layer({'input':test_tgt, 'decoder_mask':mask_tgt, 'encoder_out':test_src, 'encoder_mask':mask_src})\n",
        "out['input'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd50ff7d",
      "metadata": {
        "id": "fd50ff7d"
      },
      "outputs": [],
      "source": [
        "official_decoder_layer = nn.TransformerDecoderLayer(512, 8, 2048, batch_first=True, dropout=0)\n",
        "official_decoder_layer.self_attn.in_proj_weight.data = decoder_layer.att_block.submodule.qkv.weight.data\n",
        "official_decoder_layer.self_attn.in_proj_bias.data = decoder_layer.att_block.submodule.qkv.bias.data\n",
        "official_decoder_layer.self_attn.out_proj.weight.data = decoder_layer.att_block.submodule.out_proj.weight.data\n",
        "official_decoder_layer.self_attn.out_proj.bias.data = decoder_layer.att_block.submodule.out_proj.bias.data\n",
        "official_decoder_layer.multihead_attn.in_proj_weight.data = decoder_layer.cross_att_block.submodule.qkv.weight.data\n",
        "official_decoder_layer.multihead_attn.in_proj_bias.data = decoder_layer.cross_att_block.submodule.qkv.bias.data\n",
        "official_decoder_layer.multihead_attn.out_proj.weight.data = decoder_layer.cross_att_block.submodule.out_proj.weight.data\n",
        "official_decoder_layer.multihead_attn.out_proj.bias.data = decoder_layer.cross_att_block.submodule.out_proj.bias.data\n",
        "official_decoder_layer.linear1.weight.data = decoder_layer.mlp_block.submodule.layer[0].weight.data\n",
        "official_decoder_layer.linear1.bias.data = decoder_layer.mlp_block.submodule.layer[0].bias.data\n",
        "official_decoder_layer.linear2.weight.data = decoder_layer.mlp_block.submodule.layer[2].weight.data\n",
        "official_decoder_layer.linear2.bias.data = decoder_layer.mlp_block.submodule.layer[2].bias.data\n",
        "official_decoder_layer.norm1.weight.data = decoder_layer.att_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm1.bias.data = decoder_layer.att_block.layer_norm.bias.data\n",
        "official_decoder_layer.norm2.weight.data = decoder_layer.cross_att_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm2.bias.data = decoder_layer.cross_att_block.layer_norm.bias.data\n",
        "official_decoder_layer.norm3.weight.data = decoder_layer.mlp_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm3.bias.data = decoder_layer.mlp_block.layer_norm.bias.data\n",
        "\n",
        "# Mask for self-attentionq\n",
        "# In nn.TransformerEncoderLayer or nn.TransformerDecoderLayer, the mask is expected to be [N, Tq, Tk]\n",
        "# where N is the batch size, Tt is the query sequence length, and Ts is the key sequence length.\n",
        "head_repeated_mask_tgt = mask_tgt.unsqueeze(1).repeat(1,8,1,1).reshape(-1, mask_tgt.shape[1], mask_tgt.shape[2])\n",
        "official_decoder_output = official_decoder_layer(test_tgt, test_src, tgt_mask=head_repeated_mask_tgt==0, memory_key_padding_mask=mask_src==0)\n",
        "\n",
        "assert torch.allclose(official_decoder_output, out['input'], atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d895a7da",
      "metadata": {
        "id": "d895a7da"
      },
      "source": [
        "### Using your implementation, we can build Transformer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266428cc",
      "metadata": {
        "id": "266428cc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_layers, vocab_size):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(EncoderLayer(in_size,emb_size,mlp_size,num_head))\n",
        "    self.pos_enc = PosEncoding(emb_size, 10000)\n",
        "    self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = torch.ones([x.shape[0], x.shape[1]])\n",
        "    mask[x==0] = 0\n",
        "    temp = torch.ones_like(x)\n",
        "    result = torch.arange(x.shape[-1]).to(x.device) * temp\n",
        "    x = self.token_emb(x) + self.pos_enc(result)\n",
        "    return self.layers({'input':x, 'mask':mask})\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_layers, vocab_size):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(DecoderLayer(in_size,emb_size,mlp_size,num_head))\n",
        "    self.pos_enc = PosEncoding(emb_size, 10000)\n",
        "    self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    mask = torch.tril(torch.ones(x.shape[0], x.shape[1], x.shape[1]))\n",
        "\n",
        "    temp = torch.ones_like(x)\n",
        "    result = torch.arange(x.shape[-1]).to(x.device) * temp\n",
        "    x = self.token_emb(x) + self.pos_enc(result)\n",
        "    return self.layers({'input':x, 'decoder_mask':mask, 'encoder_out':y['input'], 'encoder_mask':y['mask']})\n",
        "\n",
        "\n",
        "class TransformerTranslator(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_enc_layers, num_dec_layers, enc_vocab_size, dec_vocab_size):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(in_size, emb_size, mlp_size, num_head, num_enc_layers, enc_vocab_size)\n",
        "    self.decoder = Decoder(in_size, emb_size, mlp_size, num_head, num_dec_layers, dec_vocab_size)\n",
        "    self.final_proj = nn.Linear(emb_size, dec_vocab_size)\n",
        "\n",
        "  def forward(self, x:torch.Tensor, y:torch.Tensor):\n",
        "    '''\n",
        "    Arguments:\n",
        "    '''\n",
        "    enc_out = self.encoder(x)\n",
        "    dec_out = self.decoder(y, enc_out)\n",
        "    return self.final_proj(dec_out['input']).softmax(dim=-1)\n",
        "\n",
        "class TransformerTrainer(Trainer):\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    super().__init__(model, optimizer, loss_fn, train_loader, valid_loader, device)\n",
        "    self.num_iter = 0\n",
        "    self._adjust_optim()\n",
        "\n",
        "  def _adjust_optim(self):\n",
        "    self.num_iter += 1\n",
        "    self.optimizer.param_groups[0]['lr'] = 512 ** (-0.5) * min(self.num_iter**(-0.5), self.num_iter*4000**(-1.5))\n",
        "\n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use variables below:\n",
        "\n",
        "    self.model (Translator/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "    '''\n",
        "\n",
        "    src, tgt_i, tgt_o = batch\n",
        "    pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
        "    pred = pack_padded_sequence(pred, pad_packed_sequence(tgt_o)[1], batch_first=True, enforce_sorted=False)\n",
        "    loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "    self._adjust_optim()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "\n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "\n",
        "\n",
        "    output:\n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "    '''\n",
        "\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "\n",
        "    validation_loss = 0\n",
        "    num_correct_guess = 0\n",
        "    num_data = 0\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(loader, leave=False):\n",
        "        src, tgt_i, tgt_o = batch\n",
        "        tgt_o = tgt_o.to(self.device)\n",
        "        pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
        "        pred = pack_padded_sequence(pred, pad_packed_sequence(tgt_o)[1], batch_first=True, enforce_sorted=False)\n",
        "        loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "\n",
        "        if isinstance(pred, PackedSequence):\n",
        "          loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "        else:\n",
        "          loss = self.loss_fn(pred, tgt_o)\n",
        "\n",
        "        validation_loss += loss.item() * len(pred.data)\n",
        "        if isinstance(pred, PackedSequence):\n",
        "          num_correct_guess += (pred.data.argmax(dim=-1) == tgt_o.data).sum().item()\n",
        "        else:\n",
        "          num_correct_guess += (pred.argmax(dim=-1) == tgt_o.data).sum().item()\n",
        "        num_data += len(pred.data)\n",
        "    return validation_loss / num_data, num_correct_guess / num_data\n",
        "\n",
        "def pad_collate(raw_batch):\n",
        "  srcs = [x[0] for x in raw_batch]\n",
        "  tgts_i = [x[1][:-1] for x in raw_batch]\n",
        "  tgts_o = [x[1][1:] for x in raw_batch]\n",
        "\n",
        "  srcs = pad_sequence(srcs, batch_first=True)\n",
        "  tgts_i = pad_sequence(tgts_i, batch_first=True)\n",
        "  tgts_o = pack_sequence(tgts_o, enforce_sorted=False)\n",
        "  return srcs, tgts_i, tgts_o\n",
        "\n",
        "single_loader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "train_loader = DataLoader(trainset, batch_size=4, collate_fn=pad_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pad_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pad_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "tfm_model = TransformerTranslator(512,512,2048,8,6,12,32000,32000)\n",
        "out = tfm_model(batch[0], batch[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42f7c9b",
      "metadata": {
        "id": "c42f7c9b"
      },
      "source": [
        "#### Transformer Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c32d97",
      "metadata": {
        "id": "36c32d97"
      },
      "outputs": [],
      "source": [
        "# download pre-trained weight\n",
        "!gdown 1nxbQPD4a2SVicLBfzrj2pRU4hAapeMmL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40d8308",
      "metadata": {
        "id": "e40d8308"
      },
      "outputs": [],
      "source": [
        "tfm_model = TransformerTranslator(512,512,2048,8,6,12,32000,32000)\n",
        "tfm_model.load_state_dict(torch.load('kor_eng_translator_tfm_model_best.pt')['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec607d0b",
      "metadata": {
        "id": "ec607d0b"
      },
      "outputs": [],
      "source": [
        "def translate_tfm(model, source_sentence, src_tokenizer, tgt_tokenizer):\n",
        "  '''\n",
        "\n",
        "  Arguments:\n",
        "    model (TranslatorAtt): Translator model with attention\n",
        "    source_sentence (str): Sentence to translate\n",
        "\n",
        "  Returns:\n",
        "    input_tokens (list): Source sentence in a list of token in token_id\n",
        "    predicted_tokens (list): Translated sentence in a list of token in token_id\n",
        "    decoded_string (str): Translated sentence in string\n",
        "\n",
        "  '''\n",
        "\n",
        "  input_tokens = src_tokenizer.encode(source_sentence)\n",
        "  input_tensor = torch.LongTensor(input_tokens).unsqueeze(0)\n",
        "  enc_out = model.encoder(input_tensor)\n",
        "\n",
        "  # Setup for 0th step\n",
        "  current_decoder_token = torch.LongTensor([[2]]) # start of sentence token\n",
        "\n",
        "  for i in range(50): # You can chage it to while True:\n",
        "    decoder_out = model.decoder(current_decoder_token, enc_out)['input']\n",
        "    logit = model.final_proj(decoder_out[0, -1])\n",
        "    selected_token = torch.argmax(logit, dim=-1)\n",
        "    current_decoder_token = torch.tensor(current_decoder_token[0].tolist() + [selected_token], dtype=torch.long).unsqueeze(0)\n",
        "    if selected_token == 3: ## end of sentence token\n",
        "      break\n",
        "  predicted_tokens = current_decoder_token.squeeze().tolist()[1:-1]\n",
        "  return input_tokens, predicted_tokens, tgt_tokenizer.decode(predicted_tokens)\n",
        "\n",
        "tfm_model.cpu()\n",
        "tfm_model.eval()\n",
        "input_tokens, pred_tokens, translated_string  = translate_tfm(tfm_model, '문장이 잘 번역되는지를 체크함으로써 모델 설계와 학습이 잘 이뤄졌는지를 확인할 수 있습니다.', src_tokenizer, tgt_tokenizer)\n",
        "print(f\"Input: {src_tokenizer.decode(input_tokens)}\")\n",
        "print(f\"Output: {tgt_tokenizer.decode(pred_tokens)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8dda25",
      "metadata": {
        "id": "ce8dda25"
      },
      "source": [
        "# Check Before Submission\n",
        "- Copy and paste your code to the downloaded ``NLP_Assignment_4.py``\n",
        "  - https://raw.githubusercontent.com/jdasam/aat3020/2025/NLP_Assignment_4.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ec2b52",
      "metadata": {
        "id": "29ec2b52"
      },
      "outputs": [],
      "source": [
        "# Run this code after copy and paste your code to NLP_Assignment_4.py\n",
        "!python NLP_Assignment_4.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}